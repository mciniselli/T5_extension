{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "evaluate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caZ61QzceIfi"
      },
      "source": [
        "### Creation of the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h1MRzBLtex2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a3eeea-625c-45b9-ed4d-df0557aaad60"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip3 install --upgrade pip\n",
        "#!pip install -qU t5\n",
        "!pip3 install git+https://github.com/google-research/text-to-text-transfer-transformer.git #extra_id_x support\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5\n",
        "\n",
        "#Set the base dir(Google cloud bucket)\n",
        "BASE_DIR = \"gs://bucket_code_completion\" \n",
        "\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"2x2\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/ef/60d7ba03b5c442309ef42e7d69959f73aacccd0d86008362a681c4698e83/pip-21.0.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.0.1\n",
            "Collecting git+https://github.com/google-research/text-to-text-transfer-transformer.git\n",
            "  Cloning https://github.com/google-research/text-to-text-transfer-transformer.git to /tmp/pip-req-build-m7v6dzvw\n",
            "  Running command git clone -q https://github.com/google-research/text-to-text-transfer-transformer.git /tmp/pip-req-build-m7v6dzvw\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (0.10.0)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (2.9.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (0.4.0)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "  Downloading mesh_tensorflow-0.1.18-py3-none-any.whl (361 kB)\n",
            "\u001b[K     |████████████████████████████████| 361 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (1.1.5)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (1.4.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14 in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (1.15.0)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.4.3-cp36-cp36m-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 13.0 MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "  Downloading tfds_nightly-4.2.0.dev202102030107-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 36.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (1.7.0+cu101)\n",
            "Collecting transformers>=2.7.0\n",
            "  Downloading transformers-4.2.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 73.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (3.4.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
            "\u001b[K     |████████████████████████████████| 883 kB 78.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (3.0.12)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 85.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel->t5==0.8.1) (2018.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers>=2.7.0->t5==0.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers>=2.7.0->t5==0.8.1) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5==0.8.1) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5==0.8.1) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.7.0->t5==0.8.1) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.7.0->t5==0.8.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.7.0->t5==0.8.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.7.0->t5==0.8.1) (2.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5==0.8.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5==0.8.1) (1.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (5.1.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (0.1.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (3.12.4)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (20.3.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (0.27.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (0.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (53.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (1.52.0)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->t5==0.8.1) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->t5==0.8.1) (0.11.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (2.10.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.3.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (2.4.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (2.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.1.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.32.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.6.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.24.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (3.3.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (4.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (3.1.0)\n",
            "Building wheels for collected packages: t5, sacremoses\n",
            "  Building wheel for t5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for t5: filename=t5-0.8.1-py3-none-any.whl size=221272 sha256=3b06ab397807c6d3cc1b50de4cb9560123ed2a0c9a2326c7630d30e8b26c4c0d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gxufwv1m/wheels/aa/e1/a1/847d16e451940b1fe89940aa88875c96ae2f7cc63e509e9226\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=82065136e57f86731b19f73b5a0c634d7ae73f9ef8736727ab06fd2a5909ac1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
            "Successfully built t5 sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, portalocker, mesh-tensorflow, transformers, tfds-nightly, tensorflow-text, sentencepiece, sacrebleu, rouge-score, t5\n",
            "Successfully installed mesh-tensorflow-0.1.18 portalocker-2.2.1 rouge-score-0.0.4 sacrebleu-1.5.0 sacremoses-0.0.43 sentencepiece-0.1.95 t5-0.8.1 tensorflow-text-2.4.3 tfds-nightly-4.2.0.dev202102030107 tokenizers-0.9.4 transformers-4.2.2\n",
            "Running on TPU: grpc://10.112.178.66:8470\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmks3SRxe-Db"
      },
      "source": [
        "### Loading of tsv files\n",
        "With this script you can load each tsv file for finetuning.\n",
        "Please be sure that the path to all tsv files are correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glLJUm1dxIiH"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_java_construct = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_java_construct.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/test_java_construct.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_java_construct = dict(train=750000, validation=106237)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-B3_th9eP5y"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_android_construct = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_android_construct.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/test_android_construct.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_android_construct = dict(train=750000, validation=100536)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u7JbyjV8GN3"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_java_block = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_java_block.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/test_java_block.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_java_block = dict(train=298470, validation=40008)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmUMU-Pg8HVm"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_android_block = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_android_block.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/test_android_block.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_android_block = dict(train=204580, validation=26978)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5lRmNWG8HuD"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_java_token = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_java_token.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/test_java_token.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_java_token = dict(train=750000, validation=219486)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF9DHJxc8IGe"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_android_token = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_android_token.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/test_android_token.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_android_token = dict(train=750000, validation=200504)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP_BYruSela-"
      },
      "source": [
        "### Preprocess of the dataset\n",
        "In this step we preprocess the dataset.  \n",
        "You have to change the path to vocab files (*vocab_model_path* and *vocab_path*)\n",
        "We're going to preprocess all the tsv file so that T5 can use them for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PobLvzL18zzR"
      },
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "\n",
        "# # Set the path of sentencepiece model and vocab files\n",
        "# # Must be the same used for the pre-trained phase\n",
        "vocab_model_path = 'gs://bucket_code_completion/T5_extension/code.model'\n",
        "vocab_path = 'gs://bucket_code_completion/T5_extension/code.vocab'\n",
        "\n",
        "\n",
        "TaskRegistry = t5.data.TaskRegistry\n",
        "TfdsTask = t5.data.TfdsTask\n",
        "\n",
        "\n",
        "def get_default_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, 100)\n",
        "\n",
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=True, required=False),\n",
        "\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=True)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn-DMH5FkSO2"
      },
      "source": [
        "JAVA CONSTRUCT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0NTLbyXvkCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57545315-c5f6-4cdf-90c4-fdc3d50b6dc2"
      },
      "source": [
        "def nq_java_construct(split, shuffle_files=True):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "   # Load lines from the text file as examples.\n",
        "\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_java_construct[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw train examples...\")\n",
        "for ex in tfds.as_numpy(nq_java_construct(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw train examples...\n",
            "{'input': b'public void updateLockdownExceptions(ManagedObjectReference _this, String[] users) throws AuthMinimumAdminPermission, RemoteException, RuntimeFault, UserNotFound { Argument[] params = new Argument[2]; params[0] = new Argument(\"_this\", \"ManagedObjectReference\", _this); params[1] = new Argument(\"users\", \"String[]\", users); getWsc().invoke( <extra_id_0>); }', 'output': b'\"UpdateLockdownExceptions\", params, null'}\n",
            "{'input': b'@Override public Collection<AgentProjectInfo> collectDependencies(String folder) { if ( <extra_id_0>){ devDependencies = findDevDependencies(folder); } File yarnLock = new File(folder + fileSeparator + YARN_LOCK); boolean yarnLockFound = yarnLock.isFile(); Collection<DependencyInfo> dependencies = new ArrayList<>(); if (yarnLockFound){ dependencies = parseYarnLock(yarnLock); } else { npmLsFailureStatus = true; } return getSingleProjectList(dependencies); }', 'output': b'!includeDevDependencies'}\n",
            "{'input': b'@Override public Collection<AgentProjectInfo> collectDependencies(String folder) { if (!includeDevDependencies){ devDependencies = findDevDependencies(folder); } File yarnLock = new File(folder + fileSeparator + YARN_LOCK); boolean yarnLockFound = yarnLock.isFile(); Collection<DependencyInfo> dependencies = new ArrayList<>(); if ( <extra_id_0>){ dependencies = parseYarnLock(yarnLock); } else { npmLsFailureStatus = true; } return getSingleProjectList(dependencies); }', 'output': b'yarnLockFound'}\n",
            "{'input': b'@Override public Collection<AgentProjectInfo> collectDependencies(String folder) { if (!includeDevDependencies){ devDependencies = findDevDependencies(folder); } File yarnLock = new File( <extra_id_0>); boolean yarnLockFound = yarnLock.isFile(); Collection<DependencyInfo> dependencies = new ArrayList<>(); if (yarnLockFound){ dependencies = parseYarnLock(yarnLock); } else { npmLsFailureStatus = true; } return getSingleProjectList(dependencies); }', 'output': b'folder + fileSeparator + YARN_LOCK'}\n",
            "{'input': b'@Override public Collection<AgentProjectInfo> collectDependencies(String folder) { if (!includeDevDependencies){ devDependencies = findDevDependencies(folder); } File yarnLock = new File(folder + fileSeparator + YARN_LOCK); boolean yarnLockFound = yarnLock.isFile(); Collection<DependencyInfo> dependencies = new ArrayList<>(); if (yarnLockFound){ dependencies = parseYarnLock(yarnLock); } else { npmLsFailureStatus = true; } return getSingleProjectList( <extra_id_0>); }', 'output': b'dependencies'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJZPQgjxKZ1"
      },
      "source": [
        "def java_construct_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['JAVA_CONSTRUCT:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3jAg8Zhx_Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d29a02-c52f-4df7-cb13-a07b84a928d0"
      },
      "source": [
        "t5.data.TaskRegistry.remove('java_construct')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"java_construct\",\n",
        "    dataset_fn=nq_java_construct,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[java_construct_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_java_construct\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f06fd342d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e71p9JIFyYHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecdc918-5035-48eb-f3b2-22d0c8ca889b"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"java_construct\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'JAVA_CONSTRUCT:public static File file(File directory, String... names) { Assert.notNull(directory, \"directorydirectory must not be null\"); if (ArrayUtil.isEmpty(names)) { return directory; } File file = directory; for (String name : names) { if ( <extra_id_0>) { file = file(file, name); } } return file; }', 'inputs': array([    3,  7641,    15,  5071, 17558,    56,  4569,    48,   181,\n",
            "         191,     5,   104,  1364,     9,    26,     4,     4,     4,\n",
            "        1886,     8,     7,   865,     4,  2623,     5,  2914,     9,\n",
            "          32,  2914,  2914,   848,   153,   198,    30,    46,    21,\n",
            "          17,   368,   306,     4,   280,     5,  3381,     8,     8,\n",
            "           7,    14,  1364,    13,     6,   181,   191,    11,  1364,\n",
            "          13,    50,    17,    31,   103,    58,  1886,     8,     7,\n",
            "          21,    17, 32099,     8,     7,   191,    11,   191,     5,\n",
            "         296,     9,   103,    10,     6,     6,    14,   191,    13,\n",
            "           6,     1], dtype=int32), 'targets_pretokenized': b'null != name', 'targets': array([ 30,  49, 103,   1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_CONSTRUCT:private void cleanUpDynamicPackages() throws IOException{ for (Service service : inputData.services.getServices()) { String serviceId = service.getId(); String packageLocation = inputData.workspaceDirectory + \"/\" + serviceId + \"/\" + StarterUtil.PACKAGE_DIR; File packageDir = new File(packageLocation); if(packageDir.exists() && packageDir.isDirectory()){ FileUtils.deleteDirectory( <extra_id_0>); log.log(Level.FINE, \"Deleted package directory for \" + serviceId + \" technology. : \" + packageLocation); } } }', 'inputs': array([    3,  7641,    15,  5071, 17558,    56,  8797,    20, 11102,\n",
            "        2431,  3809,    16,    42,   115,   683,    50,    17,   108,\n",
            "         467,    58,   297,    99,     4,  6123,     4,    33,  1640,\n",
            "          60,     7,    26, 12439,    11,   467,     4,   428,    18,\n",
            "          26,  2565,   331,    11,   297,    99,     4,  4484,   662,\n",
            "          34,  1573,    34, 12439,    34,  1573,    34,     3, 13130,\n",
            "         306,     4,  4224,    15,  3245,    13,   181,  2565,   490,\n",
            "          11,    24,   181,     5,  3703,   331,    10,    21,     5,\n",
            "        3703,   490,     4,  1090,    16,    91,  2565,   490,     4,\n",
            "        2875,  3023,  3321,     4,  7374,     5, 32099,    10,   224,\n",
            "           4,   417,     5,   377,     4,  8498,     9,    32,  3030,\n",
            "        2565,  1364,    50,    32,    34, 12439,    34,    32,     3,\n",
            "       12866,     4,    58,    32,    34,  2565,   331,    10,     6,\n",
            "           6,     6,     1], dtype=int32), 'targets_pretokenized': b'packageDir', 'targets': array([2565,  490,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_CONSTRUCT:@Override public boolean isExpired(final TicketState ticketState) { if (ticketState == null) { return true; } val now = ZonedDateTime.now(ZoneOffset.UTC); val expirationTime = ticketState.getLastTimeUsed().plus(this.timeToKillInSeconds, ChronoUnit.SECONDS); val expired = now.isAfter( <extra_id_0>); if (!expired) { return super.isExpired(ticketState); } return expired; }', 'inputs': array([    3,  7641,    15,  5071, 17558,    56,  2098,    27,    12,\n",
            "          45,     3, 19418,     5,    64, 12642,   119,  6104,   119,\n",
            "           8,     7,    21,    17,  7827,   119,    40,    30,     8,\n",
            "           7,    14,    89,    13,     6,   752,  1426,    11,     3,\n",
            "           2,  7747,     4,  2674,     5,     2, 16614,     4,  5174,\n",
            "          10,   752,  8249,   199,    11,  6104,   119,     4,  3019,\n",
            "         199,  2262,    37,  6866,     5,    75,     4,   918,   129,\n",
            "        6031,  7583,     9,     3, 28445,     4,  4184,    10,   752,\n",
            "       10197,    11,  1426,     4, 18270,     5, 32099,    10,    21,\n",
            "         124, 19971,     8,     7,    14,    52,     4, 19418,     5,\n",
            "        7827,   119,    10,     6,    14, 10197,    13,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'expirationTime', 'targets': array([8249,  199,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_CONSTRUCT:@Override public NamingRegisterRequest decode(final byte[] buf) { final AvroNamingRegisterRequest avroNamingRegisterRequest = AvroUtils.fromBytes( <extra_id_0>); return new NamingRegisterRequest( new NameAssignmentTuple(factory.getNewInstance(avroNamingRegisterRequest.getId().toString()), new InetSocketAddress(avroNamingRegisterRequest.getHost().toString(), avroNamingRegisterRequest.getPort())) ); }', 'inputs': array([    3,  7641,    15,  5071, 17558,    56,  2098,    27,    12,\n",
            "           3,  6765,  2341,   125,  2426,     5,    64,   210,    61,\n",
            "         597,     8,     7,    44,  5883,  6765,  2341,   125,     3,\n",
            "        9108,  6765,  2341,   125,    11,  5883,   217,     4,   559,\n",
            "         571,     5, 32099,    10,    14,    24,     3,  6765,  2341,\n",
            "         125,     5,    24,     3, 19308,  1613,     5,  2558,     4,\n",
            "        6406,   316,     5,  9108,  6765,  2341,   125,     4,   428,\n",
            "          37,   123,    60,     9,    24,   266,  4642,     5,  9108,\n",
            "        6765,  2341,   125,     4,  4301,    37,   123,    72,     3,\n",
            "        9108,  6765,  2341,   125,     4,  4662,   459,     3,    10,\n",
            "           6,     1], dtype=int32), 'targets_pretokenized': b'buf, AvroNamingRegisterRequest.class', 'targets': array([ 597,    9, 5883, 6765, 2341,  125,    4,   88,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_CONSTRUCT:public String buildStringToCheck(String claimIdentifier, String key, String value) throws Exception { String builtString = claimIdentifier.trim(); if (!claimIdentifier.contains(\":\")) { builtString = builtString + \":\"; } if ( <extra_id_0>) { builtString = builtString + \" key: \" + key + \" value:.*\" + value; } else { builtString = builtString + \" \" + value.replace(\"[\", \"\\\\\\\\[\").replace(\"]\", \"\\\\\\\\]\"); } return builtString; }', 'inputs': array([    3,  7641,    15,  5071, 17558,    56,  4569,    26,   501,\n",
            "          31, 10407,     5,    31, 13837,   660,     9,    26,   145,\n",
            "           9,    26,    82,     8,    42,   172,     7,    26, 13951,\n",
            "          31,    11, 13837,   660,     4,  1008,    18,    21,   124,\n",
            "       10379,   660,     4,   353, 11521,     8,     7, 13951,    31,\n",
            "          11, 13951,    31,    34,  2390,    13,     6,    21,    17,\n",
            "       32099,     8,     7, 13951,    31,    11, 13951,    31,    34,\n",
            "          32,   145,    56,    32,    34,   145,    34,    32,    82,\n",
            "          56,     4,     2,    83,    34,    82,    13,     6,    77,\n",
            "           7, 13951,    31,    11, 13951,    31,    34,    32,    32,\n",
            "          34,    82,     4,   924, 13702,     9,    32,     2,    95,\n",
            "         562,   924,    28,  2934,    32,     2,  6499,     6,    14,\n",
            "       13951,    31,    13,     6,     1], dtype=int32), 'targets_pretokenized': b'key != null', 'targets': array([145,  49,  30,   1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB-KY403kcCn"
      },
      "source": [
        "JAVA TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNi7HPiOz27q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4b9fb9a-1a15-4018-af85-3e27adcf8e40"
      },
      "source": [
        "def nq_java_token(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_java_token[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw valid examples...\")\n",
        "for ex in tfds.as_numpy(nq_java_token(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw valid examples...\n",
            "{'input': b'private void emitSelectById(JavaWriter <extra_id_0> logger.d(\"emitSelectById\"); javaWriter.beginMethod(getTargetClass(), $$GET_OBJECT_BY_ID, EnumSet.of(PUBLIC, STATIC), \"long\", \"id\", \"SQLiteDatabase\", \"db\") .emitStatement(\"Cursor cursor = db.rawQuery(\\\\\"SELECT * FROM %s WHERE %s = id\\\\\", null)\", getTableName(), idColumn.getColumnName()) .emitStatement(\"%s value = %s(cursor, db).get(0)\", getTargetClass(), $$MAP_OBJECT_FUNCTION) .emitStatement(\"cursor.close()\") .emitStatement(\"return value\") .endMethod(); }', 'output': b'javaWriter) throws IOException {'}\n",
            "{'input': b'private void emitSelectById(JavaWriter javaWriter) throws IOException { logger.d(\"emitSelectById\" <extra_id_0> javaWriter.beginMethod(getTargetClass(), $$GET_OBJECT_BY_ID, EnumSet.of(PUBLIC, STATIC), \"long\", \"id\", \"SQLiteDatabase\", \"db\") .emitStatement(\"Cursor cursor = db.rawQuery(\\\\\"SELECT * FROM %s WHERE %s = id\\\\\", null)\", getTableName(), idColumn.getColumnName()) .emitStatement(\"%s value = %s(cursor, db).get(0)\", getTargetClass(), $$MAP_OBJECT_FUNCTION) .emitStatement(\"cursor.close()\") .emitStatement(\"return value\") .endMethod(); }', 'output': b');'}\n",
            "{'input': b'private void emitSelectById(JavaWriter javaWriter) throws IOException { logger.d(\"emitSelectById\"); javaWriter.beginMethod(getTargetClass(), $$GET_OBJECT_BY_ID, EnumSet <extra_id_0> \"id\", \"SQLiteDatabase\", \"db\") .emitStatement(\"Cursor cursor = db.rawQuery(\\\\\"SELECT * FROM %s WHERE %s = id\\\\\", null)\", getTableName(), idColumn.getColumnName()) .emitStatement(\"%s value = %s(cursor, db).get(0)\", getTargetClass(), $$MAP_OBJECT_FUNCTION) .emitStatement(\"cursor.close()\") .emitStatement(\"return value\") .endMethod(); }', 'output': b'.of(PUBLIC, STATIC), \"long\",'}\n",
            "{'input': b'private void emitSelectById(JavaWriter javaWriter) throws IOException { logger.d(\"emitSelectById\"); javaWriter.beginMethod(getTargetClass(), $$GET_OBJECT_BY_ID, EnumSet.of(PUBLIC, STATIC), \"long\", \"id\", \"SQLiteDatabase\" <extra_id_0> .emitStatement(\"Cursor cursor = db.rawQuery(\\\\\"SELECT * FROM %s WHERE %s = id\\\\\", null)\", getTableName(), idColumn.getColumnName()) .emitStatement(\"%s value = %s(cursor, db).get(0)\", getTargetClass(), $$MAP_OBJECT_FUNCTION) .emitStatement(\"cursor.close()\") .emitStatement(\"return value\") .endMethod(); }', 'output': b', \"db\")'}\n",
            "{'input': b'private void emitSelectById(JavaWriter javaWriter) throws IOException { logger.d(\"emitSelectById\"); javaWriter.beginMethod(getTargetClass(), $$GET_OBJECT_BY_ID, EnumSet.of(PUBLIC, STATIC), \"long\", \"id\", \"SQLiteDatabase\", \"db\") .emitStatement(\"Cursor cursor = db.rawQuery(\\\\\"SELECT * FROM %s WHERE %s = id\\\\\", null)\" <extra_id_0> getTableName(), idColumn.getColumnName()) .emitStatement(\"%s value = %s(cursor, db).get(0)\", getTargetClass(), $$MAP_OBJECT_FUNCTION) .emitStatement(\"cursor.close()\") .emitStatement(\"return value\") .endMethod(); }', 'output': b','}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvDAbgNY0B4Y"
      },
      "source": [
        "def java_token_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['JAVA_TOKEN:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mm6AQfw0INC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d31dd681-0308-4472-bc67-a015ec850544"
      },
      "source": [
        "t5.data.TaskRegistry.remove('java_token')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"java_token\",\n",
        "    dataset_fn=nq_java_token,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[java_token_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_java_token\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f06e6ed4da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnf25qt10Wkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac85325-73bb-49e5-fad0-56061ff7fccd"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"java_token\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'inputs_pretokenized': b'JAVA_TOKEN:public boolean isRenderable() { if (isRenderable == null) { String extension = getNameExtension(); isRenderable = isTyped() || (isText() && (HTML.equals(extension) )); } return isRenderable <extra_id_0> }', 'inputs': array([    3,  7641,    15,  2591,    56,  4569,    45,    69, 22326,\n",
            "          16,     7,    21,    17,   112, 22326,    40,    30,     8,\n",
            "           7,    26,  1884,    11,  1975,   867,    18,    69, 22326,\n",
            "          11,    69,  4947,    16,     3,     2,    17,   112,   204,\n",
            "          16,    91,    17,  3536,     4,   117,     5,  3645,     8,\n",
            "           3,    79,     6,    14,    69, 22326, 32099,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b';', 'targets': array([ 3, 13,  1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_TOKEN:public String getRequestLogMessage(Throwable e, HttpServletRequest <extra_id_0> Throwable cause = getRootCause(e); String exceptionName = cause.getClass().getSimpleName(); return getRequestLogMessage(exceptionName, request, cause.getMessage()); }', 'inputs': array([    3,  7641,    15,  2591,    56,  4569,    26,  6080, 13657,\n",
            "           5,   832,    57,     9,  1197,   125, 32099,   668,   681,\n",
            "          11,     3, 25421,     5,   110,    10,    26,   733,    66,\n",
            "          11,   681,     4,   398,    37,  1673,    18,    14,  6080,\n",
            "       13657,     5,  1315,    66,     9,   190,     9,   681,     4,\n",
            "         429,    39,     6,     1], dtype=int32), 'targets_pretokenized': b'request) {', 'targets': array([190,   8,   7,   1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_TOKEN:public void addPropertySoftDeleteToMutation(Mutation m, Property property) { Preconditions.checkNotNull(m, \"mutation cannot be null\"); Preconditions.checkNotNull(property, \"property cannot be null\"); Text columnQualifier = KeyHelper.getColumnQualifierFromPropertyColumnQualifier(property, getNameSubstitutionStrategy()); ColumnVisibility columnVisibility = visibilityToAccumuloVisibility(property <extra_id_0> m.put(AccumuloElement.CF_PROPERTY_SOFT_DELETE, columnQualifier, columnVisibility, currentTimeMillis(), AccumuloElement.SOFT_DELETE_VALUE); }', 'inputs': array([    3,  7641,    15,  2591,    56,  4569,    20, 13832,  7901,\n",
            "        1027,   129,  7429,     5,  7429,    54,     9,  2286,   698,\n",
            "           8,     7,  1872,     4,  2673,     5,    87,     9,    32,\n",
            "       20930,  1267,   198,    30,    46,  1872,     4,  2673,     5,\n",
            "        1238,     9,    32,  1238,  1267,   198,    30,    46,  2187,\n",
            "         763,     2,  3465,    11,     3,   133,   327,     4,  4033,\n",
            "           2,  3465,   253,   220,   438,     2,  3465,     5,  1238,\n",
            "           9,  1975,  8857,   894,    39,  4455,  2704,   763,  2704,\n",
            "          11,  7703,   129, 20510,  2704,     5,  1238, 32099,    54,\n",
            "           4,   120,     5, 20510,   177,     4,  5740,    15,  1527,\n",
            "           2,    15, 27216,    15,  2387,     9,   763,     2,  3465,\n",
            "           9,   763,  2704,     9,   202,  1167,    72,     3, 20510,\n",
            "         177,     4, 27216,    15,  2387,    15,   796,    10,     6,\n",
            "           1], dtype=int32), 'targets_pretokenized': b'.getVisibility());', 'targets': array([   3,    4, 9420,   39,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_TOKEN:protected Collection<Class<? extends JsonNode>> getAllowedJsonTypes(Overlay<?> value) { if (allowedJsonTypes == null) { createAllowedJsonTypes <extra_id_0> } JsonOverlay<?> overlay = value.getOverlay(); return allowedJsonTypes .get(overlay instanceof PropertiesOverlay ? PropertiesOverlay.class : overlay.getClass()); }', 'inputs': array([    3,  7641,    15,  2591,    56, 18728,   485,    25,   114,\n",
            "          25,     2,   324,  6121,   243,    41,  2107,   974,   615,\n",
            "           5,  2274,    25,     2,    29,    82,     8,     7,    21,\n",
            "          17,  6402,   974,   615,    40,    30,     8,     7,   131,\n",
            "        2107,   974,   615, 32099,     6,  2121,  2274,    25,     2,\n",
            "          29,  6356,    11,    82,     4,    33,  2274,    18,    14,\n",
            "        2447,   974,   615,     3,     4,    33,     5, 11824,   166,\n",
            "        1499,  2274,     3,     2,  1499,  2274,     4,    88,    58,\n",
            "        6356,     4,   398,    39,     6,     1], dtype=int32), 'targets_pretokenized': b'();', 'targets': array([ 3, 18,  1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_TOKEN:public boolean isRenderable() { if (isRenderable == null) { String extension = getNameExtension(); isRenderable = isTyped() || (isText() && (HTML.equals <extra_id_0> } return isRenderable; }', 'inputs': array([    3,  7641,    15,  2591,    56,  4569,    45,    69, 22326,\n",
            "          16,     7,    21,    17,   112, 22326,    40,    30,     8,\n",
            "           7,    26,  1884,    11,  1975,   867,    18,    69, 22326,\n",
            "          11,    69,  4947,    16,     3,     2,    17,   112,   204,\n",
            "          16,    91,    17,  3536,     4,   117, 32099,     6,    14,\n",
            "          69, 22326,    13,     6,     1], dtype=int32), 'targets_pretokenized': b'(extension) ));', 'targets': array([  17, 3645,    8,    3,   79,    1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIe-u5l9ke6x"
      },
      "source": [
        "JAVA BLOCK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr0TT18ejMtY",
        "outputId": "657bdaf1-36e4-4e5c-f7f7-70329f0c146f"
      },
      "source": [
        "def nq_java_block(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_java_block[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw valid examples...\")\n",
        "for ex in tfds.as_numpy(nq_java_block(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw valid examples...\n",
            "{'input': b'public String capitalizeWords(final Object target) { if (target == null) <extra_id_0> return StringUtils.capitalizeWords(target); }', 'output': b'{ return null; }'}\n",
            "{'input': b'@SuppressWarnings(\"unchecked\") public <T extends CMAResource> T setEnvironmentId(String environmentId) { if (getSystem().getEnvironment() == null) <extra_id_0> getSystem().environment.setId(environmentId); return (T) this; }', 'output': b'{ getSystem().environment = new CMALink(CMAType.Environment); }'}\n",
            "{'input': b'@Deprecated public static byte[] convertLongToVarInt(long value) { ByteBuffer longBB = ByteBuffer.allocate(EthereumUtil.LONG_SIZE); longBB.putLong(value); byte[] result = longBB.array(); int leadingZeros=0; for (int i=0;i<result.length;i++) { if (result[i]==0) <extra_id_0> else { break; } } return Arrays.copyOfRange(result, leadingZeros, result.length); }', 'output': b'{ leadingZeros++; }'}\n",
            "{'input': b'@Deprecated public static byte[] convertLongToVarInt(long value) { ByteBuffer longBB = ByteBuffer.allocate(EthereumUtil.LONG_SIZE); longBB.putLong(value); byte[] result = longBB.array(); int leadingZeros=0; for (int i=0;i<result.length;i++) { if (result[i]==0) { leadingZeros++; } else <extra_id_0> } return Arrays.copyOfRange(result, leadingZeros, result.length); }', 'output': b'{ break; }'}\n",
            "{'input': b'public void sendMessages(byte[] data) { try <extra_id_0> catch (final IOException exception) { Verbose.exception(exception); } }', 'output': b'{ out.write(data); out.flush(); }'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq0uLYNTjM9z"
      },
      "source": [
        "def java_block_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['JAVA_BLOCK:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji4u8yhqjNER",
        "outputId": "fe2f52b7-20e0-4bae-c18a-d281ede152e1"
      },
      "source": [
        "t5.data.TaskRegistry.remove('java_block')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"java_block\",\n",
        "    dataset_fn=nq_java_block,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[java_block_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_java_block\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f06fd32e0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG09lDZdjNKr",
        "outputId": "cbc1d237-4f87-493b-ec82-e6197093a7cb"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"java_block\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'inputs_pretokenized': b'JAVA_BLOCK:@Override public V g(K key) throws PageException { V v = map.get(key); if (v == NULL) return null; if (v == null) <extra_id_0> return v; }', 'inputs': array([    3,  7641,    15,  3517,    56,  2098,    27,    12,   862,\n",
            "         841,     5,   390,   145,     8,    42,  3031,    38,     7,\n",
            "         862,   284,    11,   342,     4,    33,     5,   158,    10,\n",
            "          21,    17,   291,    40,  5614,     8,    14,    30,    13,\n",
            "          21,    17,   291,    40,    30,     8, 32099,    14,   284,\n",
            "          13,     6,     1], dtype=int32), 'targets_pretokenized': b'throw invalidKey(this, key, false);', 'targets': array([  78, 2723,  133,    5,   75,    9,  145,    9,   76,   10,    1],\n",
            "      dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_BLOCK:public void set(String paramter, Object value) { ChangeLogParameter param = findParameter(paramter, null); if (param == null) <extra_id_0> }', 'inputs': array([    3,  7641,    15,  3517,    56,  4569,    20,    55,     5,\n",
            "          31,   988,  4441,     9,   102,    82,     8,     7,     3,\n",
            "       15277,   513,   988,    11,   503,   513,     5,  1560,  4441,\n",
            "           9,    30,    10,    21,    17,  1560,    40,    30,     8,\n",
            "       32099,     6,     1], dtype=int32), 'targets_pretokenized': b'{ changeLogParameters.add(new ChangeLogParameter(paramter, value)); }', 'targets': array([    7, 26078,   542,     4,    67,     5,    74,     3, 15277,\n",
            "         513,     5,  1560,  4441,     9,    82,    79,     6,     1],\n",
            "      dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_BLOCK:public BigMoney plusMinor(long amountToAdd) { if (amountToAdd == 0) <extra_id_0> BigDecimal newAmount = amount.add(BigDecimal.valueOf(amountToAdd, currency.getDecimalPlaces())); return BigMoney.of(currency, newAmount); }', 'inputs': array([    3,  7641,    15,  3517,    56,  4569, 30085,  8940,  8170,\n",
            "           5,   288,     3, 28214,     8,     7,    21,    17, 28214,\n",
            "          40,   178, 32099,  2322,    24,  1539,    11,  1453,     4,\n",
            "          67,     5,  3191,     4,   510,     5, 28214,     9,  5599,\n",
            "           4,    33,  4899, 15331,   366,    14, 30085,     4,   579,\n",
            "           5,  7110,     9,    24,  1539,    10,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'{ return this; }', 'targets': array([ 7, 14, 23, 13,  6,  1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_BLOCK:public static boolean valueChanged(Boolean before, Boolean after) { if ((before == null && after == null) || after == null) { return false; } if (before == null) <extra_id_0> return !before.equals(after); }', 'inputs': array([    3,  7641,    15,  3517,    56,  4569,    48,    45, 11940,\n",
            "           5,   649,  1048,     9,   536,  1056,     8,     7,    21,\n",
            "          17,     5,  3694,    40,    30,    91,  1056,    40,    30,\n",
            "           8,     3,     2,  1056,    40,    30,     8,     7,    14,\n",
            "          76,    13,     6,    21,    17,  3694,    40,    30,     8,\n",
            "       32099,    14,   232,  3694,     4,   117,     5,  3428,    10,\n",
            "           6,     1], dtype=int32), 'targets_pretokenized': b'{ return true; }', 'targets': array([ 7, 14, 89, 13,  6,  1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_BLOCK:void setAttribute(XMLName xmlName, Object value) { if (!isElement()) <extra_id_0> if (xmlName.uri() == null && xmlName.localName().equals(\"*\")) { throw ScriptRuntime.typeError(\"@* assignment not supported.\"); } this.node.setAttribute(xmlName.toQname(), ScriptRuntime.toString(value)); }', 'inputs': array([    3,  7641,    15,  3517,    56,  9795, 10215,     5,     2,\n",
            "         575,    66,  1850,    66,     9,   102,    82,     8,     7,\n",
            "          21,   124,   112,   177,    60, 32099,    21,    17,   755,\n",
            "          66,     4,   800,    16,    40,    30,    91,  1850,    66,\n",
            "           4, 13923,    37,   117,    28,     2,   425,     7,    78,\n",
            "        5634,  2158,     4,   171,   287,  7969,     2,  5498,   153,\n",
            "        1269,     4,    46,     6,    23,     4,   391,     4,  2413,\n",
            "           5,   755,    66,     4,   314,     2,    98,    72,  5634,\n",
            "        2158,     4,   123,     5,   122,    79,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'throw new IllegalStateException(\"Can only set attributes on elements.\");', 'targets': array([  78,   24,  857,   38,   28, 1825,  970,   55, 1389,  170, 1648,\n",
            "          4,   46,    1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz_Gxq_4khQt"
      },
      "source": [
        "ANDROID CONSTRUCT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNwnQAMVjNdy",
        "outputId": "c957b717-2bf5-408b-f4a1-102d5b18c9d8"
      },
      "source": [
        "def nq_android_construct(split, shuffle_files=True):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "   # Load lines from the text file as examples.\n",
        "\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_android_construct[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw train examples...\")\n",
        "for ex in tfds.as_numpy(nq_android_construct(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw train examples...\n",
            "{'input': b'private void writeToFile(final String content) { if ( <extra_id_0>) { return; } try { System.err.println(content); File f = new File(OUTPUT_FILE); f.delete(); FileWriter fstream = new FileWriter(OUTPUT_FILE); BufferedWriter out = new BufferedWriter(fstream); out.write(content); out.close(); } catch (Exception e) { e.printStackTrace(); } }', 'output': b'!WRITE_TO_FILE'}\n",
            "{'input': b'private void writeToFile(final String content) { if (!WRITE_TO_FILE) { return; } try { System.err.println(content); File f = new File(OUTPUT_FILE); f.delete(); FileWriter fstream = new FileWriter(OUTPUT_FILE); BufferedWriter out = new BufferedWriter(fstream); out.write(content); out.close(); } catch ( <extra_id_0>) { e.printStackTrace(); } }', 'output': b'Exception e'}\n",
            "{'input': b'private void writeToFile(final String content) { if (!WRITE_TO_FILE) { return; } try { System.err.println(content); File f = new File( <extra_id_0>); f.delete(); FileWriter fstream = new FileWriter(OUTPUT_FILE); BufferedWriter out = new BufferedWriter(fstream); out.write(content); out.close(); } catch (Exception e) { e.printStackTrace(); } }', 'output': b'OUTPUT_FILE'}\n",
            "{'input': b'private void writeToFile(final String content) { if (!WRITE_TO_FILE) { return; } try { System.err.println( <extra_id_0>); File f = new File(OUTPUT_FILE); f.delete(); FileWriter fstream = new FileWriter(OUTPUT_FILE); BufferedWriter out = new BufferedWriter(fstream); out.write(content); out.close(); } catch (Exception e) { e.printStackTrace(); } }', 'output': b'content'}\n",
            "{'input': b'private void writeToFile(final String content) { if (!WRITE_TO_FILE) { return; } try { System.err.println(content); File f = new File(OUTPUT_FILE); f.delete(); FileWriter fstream = new FileWriter(OUTPUT_FILE); BufferedWriter out = new BufferedWriter(fstream); out.write( <extra_id_0>); out.close(); } catch (Exception e) { e.printStackTrace(); } }', 'output': b'content'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sBEViP5jNja"
      },
      "source": [
        "def android_construct_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['ANDROID_CONSTRUCT:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP_VXjC6jNpp",
        "outputId": "7a779ecc-035d-4132-95a9-557f33e56f60"
      },
      "source": [
        "t5.data.TaskRegistry.remove('android_construct')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"android_construct\",\n",
        "    dataset_fn=nq_android_construct,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[android_construct_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_android_construct\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f06e6fc2a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOWFYL7KjNwd",
        "outputId": "1d39cd47-4e1f-4e9c-caa4-ebecc34f77b2"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"android_construct\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'inputs_pretokenized': b'ANDROID_CONSTRUCT:private void buildCursorMapForExistingCollaborators(Collaborator collaborator) { if (collaborator == null) { return; } IndexReference ref = (IndexReference) mCursors.get(collaborator.getSessionId()); if (ref == null) { return; } Set<Collaborator> row = new HashSet<>(); mRowToCollaborators.put( <extra_id_0>); row.add(collaborator); }', 'inputs': array([    3, 16446,    15,  5071, 17558,    56,  8797,    20,   501,\n",
            "        1123,   100,   203,  4001, 18991,    22,     5, 18991,     3,\n",
            "       19359,     8,     7,    21,    17, 19359,    40,    30,     8,\n",
            "           7,    14,    13,     6,     3,   163,   478,  1359,    11,\n",
            "          17,   163,   478,     8,  8808,    22,     4,    33,     5,\n",
            "       19359,     4, 13816,    39,    21,    17,  1391,    40,    30,\n",
            "           8,     7,    14,    13,     6,   300,    25, 18991,    29,\n",
            "         722,    11,    24,   978,   447,    54,   634,   129, 18991,\n",
            "          22,     4,   120,     5, 32099,    10,   722,     4,    67,\n",
            "           5, 19359,    10,     6,     1], dtype=int32), 'targets_pretokenized': b'ref.getIndex(), row', 'targets': array([1359,    4, 4488,   72,  722,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_CONSTRUCT:public ChatMessagesAdapter(User user, InfiniteFireArray snapshots) { super(snapshots, 0, 0); this.user = user; this.recordAudioFragment = new RecordAudioFragment(); setHasStableIds( <extra_id_0>); }', 'inputs': array([    3, 16446,    15,  5071, 17558,    56,  4569,     3, 11671,\n",
            "          22,   442,     5,   246,   269,     9,     3, 25580,  4967,\n",
            "         368,  3983,    22,     8,     7,    52,     5,  6376,    22,\n",
            "           9,   312,   522,    23,     4,   418,    11,   269,    13,\n",
            "          23,     4,  1717,  2197,   619,    11,    24,     3,   504,\n",
            "        2197,   619,    18,  9509, 21348,   888,     5, 32099,    10,\n",
            "           6,     1], dtype=int32), 'targets_pretokenized': b'true', 'targets': array([89,  1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_CONSTRUCT:protected Response<String> parseNetworkResponse(NetworkResponse response) { try { Log.i(\"eswaraj\", \"Status Code = \"+response.statusCode); String json = new String(response.data, HttpHeaderParser.parseCharset(response.headers)); Log.i(\"eswaraj\", \"json response = \"+json); return Response.success( json, HttpHeaderParser.parseCacheHeaders(response)); } catch (UnsupportedEncodingException e) { return Response.error( <extra_id_0>); } }', 'inputs': array([    3, 16446,    15,  5071, 17558,    56, 18728,  1185,    25,\n",
            "          31,    29,   545,  1062,   164,     5,  1062,   164,   250,\n",
            "           8,     7,    93,     7,   319,     4,    86,    28,   692,\n",
            "        8903,   184,   523,    43,    32,   221,     3,   359,    11,\n",
            "        1113,   762,     4,  6563,    10,    26,   884,    11,    24,\n",
            "          26,     5,   762,     4,   258,     9,     3, 16294,   519,\n",
            "           4,   655,  3662,     5,   762,     4,  3251,    79,   319,\n",
            "           4,    86,    28,   692,  8903,   184,   523,    43,    32,\n",
            "         896,   250,    11,  1113,   896,    10,    14,  1185,     4,\n",
            "        2309,     5,   884,     9,     3, 16294,   519,     4,   655,\n",
            "         307,  1664,     5,   762,    79,     6,    97,    17,  5912,\n",
            "        4128,    57,     8,     7,    14,  1185,     4,   282,     5,\n",
            "       32099,    10,     6,     6,     1], dtype=int32), 'targets_pretokenized': b'new ParseError(e)', 'targets': array([   24,     3, 19271,     5,   110,     8,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_CONSTRUCT:private void setActivity(boolean activity) { activityIndicatorView.setVisibility(activity ? VISIBLE : GONE); activityOverlayView.setVisibility( <extra_id_0>); }', 'inputs': array([    3, 16446,    15,  5071, 17558,    56,  8797,    20,    55,\n",
            "         435,     5,   257,  1028,     8,     7,  1028,  2176,   143,\n",
            "           4,  1549,     5,  1338,     3,     2,     3,  2314,    58,\n",
            "           3,  2932,    10,  1028,  2274,   143,     4,  1549,     5,\n",
            "       32099,    10,     6,     1], dtype=int32), 'targets_pretokenized': b'activity ? VISIBLE : GONE', 'targets': array([1028,    3,    2,    3, 2314,   58,    3, 2932,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_CONSTRUCT:public JSONObject getTotalResulData() throws JSONException{ JSONObject itg=null; if(this.fileExist(this.itgSendConfFile)){ itg=this.getJSONFromFile( <extra_id_0>); JSONObject obj=itg.getJSONObject(\"Total Result\"); return obj; } return null; }', 'inputs': array([    3, 16446,    15,  5071, 17558,    56,  4569,  1327,  5673,\n",
            "        1977,  3655,    99,    16,    42,  3920,   683,  1327,   305,\n",
            "         423,   161,   180,    13,    21,     5,    75,     4,   296,\n",
            "        4852,     5,    75,     4,  1256,   423,  1674,  1503,   104,\n",
            "        3995,   305,   423,   161,    75,     4,    33,   825,  5879,\n",
            "           5, 32099,    10,  1327,   409,   161,  1256,   423,     4,\n",
            "        9412,    28,  2134,  3026,    46,    14,   409,    13,     6,\n",
            "          14,    30,    13,     6,     1], dtype=int32), 'targets_pretokenized': b'this.itgSendConfFile', 'targets': array([  23,    4, 1256,  423, 1674, 1503,  104,    1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ9H1D-KngJy"
      },
      "source": [
        "ANDROID TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgi_2yd-nb4Y",
        "outputId": "2722b6db-2fc0-4be7-cf3d-ec4df4889dc4"
      },
      "source": [
        "def nq_android_token(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_android_token[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw valid examples...\")\n",
        "for ex in tfds.as_numpy(nq_android_token(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw valid examples...\n",
            "{'input': b'public boolean initialize <extra_id_0> if (bluetoothManager == null) { bluetoothManager = (BluetoothManager) getSystemService(Context.BLUETOOTH_SERVICE); if (bluetoothManager == null) { Log.e(TAG, \"Unable to initialize BluetoothManager.\"); return false; } } adapter = bluetoothManager.getAdapter(); if (adapter == null) { Log.e(TAG, \"Unable to obtain a BluetoothAdapter.\"); return false; } return true; }', 'output': b'() {'}\n",
            "{'input': b'public boolean initialize() { if (bluetoothManager == <extra_id_0> bluetoothManager = (BluetoothManager) getSystemService(Context.BLUETOOTH_SERVICE); if (bluetoothManager == null) { Log.e(TAG, \"Unable to initialize BluetoothManager.\"); return false; } } adapter = bluetoothManager.getAdapter(); if (adapter == null) { Log.e(TAG, \"Unable to obtain a BluetoothAdapter.\"); return false; } return true; }', 'output': b'null) {'}\n",
            "{'input': b'public boolean initialize() { if (bluetoothManager == null) { bluetoothManager = (BluetoothManager) getSystemService(Context.BLUETOOTH_SERVICE <extra_id_0> if (bluetoothManager == null) { Log.e(TAG, \"Unable to initialize BluetoothManager.\"); return false; } } adapter = bluetoothManager.getAdapter(); if (adapter == null) { Log.e(TAG, \"Unable to obtain a BluetoothAdapter.\"); return false; } return true; }', 'output': b');'}\n",
            "{'input': b'public boolean initialize() { if (bluetoothManager == null) { bluetoothManager = (BluetoothManager) getSystemService(Context.BLUETOOTH_SERVICE); if (bluetoothManager <extra_id_0> Log.e(TAG, \"Unable to initialize BluetoothManager.\"); return false; } } adapter = bluetoothManager.getAdapter(); if (adapter == null) { Log.e(TAG, \"Unable to obtain a BluetoothAdapter.\"); return false; } return true; }', 'output': b'== null) {'}\n",
            "{'input': b'public boolean initialize() { if (bluetoothManager == null) { bluetoothManager = (BluetoothManager) getSystemService(Context.BLUETOOTH_SERVICE); if (bluetoothManager == null) { Log.e(TAG, \"Unable to initialize BluetoothManager.\" <extra_id_0> return false; } } adapter = bluetoothManager.getAdapter(); if (adapter == null) { Log.e(TAG, \"Unable to obtain a BluetoothAdapter.\"); return false; } return true; }', 'output': b');'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcKSDpJrnb_X"
      },
      "source": [
        "def android_token_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['ANDROID_TOKEN:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvwEpdZIjN_9",
        "outputId": "a72decbd-3b77-4170-dc0c-685932e968eb"
      },
      "source": [
        "t5.data.TaskRegistry.remove('android_token')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"android_token\",\n",
        "    dataset_fn=nq_android_token,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[android_token_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_android_token\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f06e6f9b208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6z53ZuUpT3f",
        "outputId": "45e10077-e9ad-4985-b7af-3eccccc61a6d"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"android_token\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'ANDROID_TOKEN:public List<GalleryObject> loadStoredGalleryObjects(GalleryObject parent) { if (parent == null) { return null; } if (!parent.equals(mStoredGalleryObject)) { return <extra_id_0> } return mGalleryChildObjects; }', 'inputs': array([    3, 16446,    15,  2591,    56,  4569,    85,    25, 11060,\n",
            "          96,    29,   551,  4438, 11060,  1620,     5, 11060,    96,\n",
            "         340,     8,     7,    21,    17,   614,    40,    30,     8,\n",
            "           7,    14,    30,    13,     6,    21,   124,   614,     4,\n",
            "         117,     5,    87,  4438, 11060,    96,     8,     8,     7,\n",
            "          14, 32099,     6,    14,    54, 11060,   995,  1620,    13,\n",
            "           6,     1], dtype=int32), 'targets_pretokenized': b'null;', 'targets': array([30, 13,  1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_TOKEN:protected void onPostCreate(Bundle savedInstanceState) { super.onPostCreate(savedInstanceState <extra_id_0> setContentView(R.layout.activity_preferences); setupSimplePreferencesScreen(); }', 'inputs': array([    3, 16446,    15,  2591,    56, 18728,    20,     3, 29679,\n",
            "           5,   674,  1914,   119,     8,     7,    52,     4, 29679,\n",
            "           5,  2499,   119, 32099,  6138,     5,   144,     4,  1179,\n",
            "           4,  1338,    15,  7927,    10,   941,  1422,  1355,  1260,\n",
            "          18,     6,     1], dtype=int32), 'targets_pretokenized': b');', 'targets': array([ 3, 10,  1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_TOKEN:public Builder setCongestionLevel(com.echo5bravo.gtfs.GtfsRealtime.VehiclePosition <extra_id_0> if (value == null) { throw new NullPointerException(); } bitField0_ |= 0x00000080; congestionLevel_ = value; onChanged(); return this; }', 'inputs': array([    3, 16446,    15,  2591,    56,  4569,  1018,    55, 31663,\n",
            "         377,     5,   653,     4, 15279,     2, 21290,  6549,     4,\n",
            "       27697,     4,  7435,     4,  5421,   392, 32099,    21,    17,\n",
            "         122,    40,    30,     8,     7,    78,    24,  2024,    38,\n",
            "          18,     6,  4787,  2022,     3,     2,   161,   157,   138,\n",
            "        7025,     2,  2986,  2059,  5963,    22,  9474,   377,    15,\n",
            "          11,    82,    13,  3533,    18,    14,    23,    13,     6,\n",
            "           1], dtype=int32), 'targets_pretokenized': b'.CongestionLevel value) {', 'targets': array([    3,     4, 31663,   377,    82,     8,     7,     1],\n",
            "      dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_TOKEN:public void searchSuggestionsTest() { openSearchView(); ViewInteraction searchAutoComplete = typeQuery(\"heavy\"); checkSearchSuggestions(\"heavy\", \"heavyset\", \"heavyweight\" <extra_id_0> searchAutoComplete.perform(typeText(\"s\")); checkSearchSuggestions(\"heavyset\"); searchAutoComplete.perform(typeText(\"z\")); checkSingleRootView(mActivityTestRule.getActivity()); }', 'inputs': array([    3, 16446,    15,  2591,    56,  4569,    20,  1014,  4268,\n",
            "         231,    16,     7,  1084, 15233,    18,   886,  3138,  1014,\n",
            "       18989,    11,   147,     2,   179,    28,   773, 26871,    46,\n",
            "         272,   758,  4268,    28,   773, 26871,    43,    32,   773,\n",
            "       26871,    63,    43,    32,   773, 26871,  5172,    83, 32099,\n",
            "        1014, 18989,     4,  4175,     5,   171,   204,    28,    22,\n",
            "         295,   272,   758,  4268,    28,   773, 26871,    63,    46,\n",
            "        1014, 18989,     4,  4175,     5,   171,   204,    28,   652,\n",
            "         295,   272,  1623,   795,   143,     5,    87, 28360,     4,\n",
            "        1926,    39,     6,     1], dtype=int32), 'targets_pretokenized': b', \"heavyweights\");', 'targets': array([    3,     9,    32,   773, 26871, 21301,    46,     1],\n",
            "      dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_TOKEN:public boolean onCreateOptionsMenu(Menu menu) { mainMenu = <extra_id_0> mainMenuDelegate.onCreateOptionsMenu(menu); MenuItem downloadItem = mainMenu.add(0, DOWNLOAD_ITEM_ID, mainMenu.size(), R.string.menu_item_download_all); downloadItem.setIcon(R.drawable.av_download); downloadItem.setShowAsAction(MenuItem.SHOW_AS_ACTION_IF_ROOM); return true; }', 'inputs': array([    3, 16446,    15,  2591,    56,  4569,    45,  1675,  2856,\n",
            "           5,  1061,  1408,     8,     7,   868,  1061,    11, 32099,\n",
            "         868,  1061,  1638,     4,  1869,  2856,     5,  1601,    10,\n",
            "        6624,  2237,   169,    11,   868,  1061,     4,    67,   537,\n",
            "           3,  9798,    15,  2897,    15,   142,     9,   868,  1061,\n",
            "           4,   134,    72,   544,     4,   383,     4,  1601,    15,\n",
            "         440,    15,  3171,    15,  1241,    10,  2237,   169,     4,\n",
            "        6896,     5,   144,     4,  2471,     4,  4699,    15,  3171,\n",
            "          10,  2237,   169,     4, 25177,     5,  1938,     4,  4109,\n",
            "          15,  2663,    15,  1023,    15,  3933,    15, 21982,    10,\n",
            "          14,    89,    13,     6,     1], dtype=int32), 'targets_pretokenized': b'menu;', 'targets': array([1408,   13,    1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q3xYhUwoBC6"
      },
      "source": [
        "ANDROID BLOCK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqvJtXAKjOFn",
        "outputId": "7ae3df64-4943-423e-96bb-a566c63669f2"
      },
      "source": [
        "def nq_android_block(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_android_block[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw valid examples...\")\n",
        "for ex in tfds.as_numpy(nq_android_block(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw valid examples...\n",
            "{'input': b'public void onStackEmpty() { cancelAutoLove(); if (progressDialog == null || !progressDialog.isShowing()) <extra_id_0> }', 'output': b'{ progressDialog = ProgressDialog.show(this, \"Out of Cats\", \"Searching for more, Please wait...\"); startLoading(); }'}\n",
            "{'input': b'private static List<HeroAndAdvantages> loadHeroes(SQLiteDatabase db) { List<HeroAndAdvantages> heroes = new ArrayList<>(); Cursor c = db.rawQuery(\"SELECT * FROM Heroes\", null); c.moveToFirst(); while (!c.isAfterLast()) <extra_id_0> c.close(); return heroes; }', 'output': b'{ heroes.add(new HeroAndAdvantages(c)); c.moveToNext(); }'}\n",
            "{'input': b'private String getLinkByRelation(String relation) { for (Link l : link) { if (l.getRel().equals(relation)) <extra_id_0> } return null; }', 'output': b'{ return l.getUri(); }'}\n",
            "{'input': b'public static boolean isBundleValid(final Bundle bundle) { if (null == bundle) <extra_id_0> if (bundle.getInt(BUNDLE_EXTRA_INT_VERSION_CODE, -1) == -1) { return false; } if (isNullOrEmpty(bundle.getString(BUNDLE_EXTRA_STRING_FILTER))) { Timber.e(\"Invalid %s\", BUNDLE_EXTRA_STRING_FILTER); return false; } return true; }', 'output': b'{ Timber.e(\"bundle is null\"); return false; }'}\n",
            "{'input': b'public static boolean isBundleValid(final Bundle bundle) { if (null == bundle) { Timber.e(\"bundle is null\"); return false; } if (bundle.getInt(BUNDLE_EXTRA_INT_VERSION_CODE, -1) == -1) <extra_id_0> if (isNullOrEmpty(bundle.getString(BUNDLE_EXTRA_STRING_FILTER))) { Timber.e(\"Invalid %s\", BUNDLE_EXTRA_STRING_FILTER); return false; } return true; }', 'output': b'{ return false; }'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvADtjcHoGCM"
      },
      "source": [
        "def android_block_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['ANDROID_BLOCK:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY6Cr6OyoGLR",
        "outputId": "dc5d185d-80e0-48ca-824d-f162d05302e1"
      },
      "source": [
        "t5.data.TaskRegistry.remove('android_block')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"android_block\",\n",
        "    dataset_fn=nq_android_block,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[android_block_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_android_block\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f06e6fe3d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIpCwr6FjOMd",
        "outputId": "5b17a172-3cf4-42f2-d19b-ffaa387ddc61"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"android_block\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'inputs_pretokenized': b'ANDROID_BLOCK:public NotificationItemAdapter(Context context, List<NotificationItem> objects) { super(context, 0, objects); this.sort(new Comparator<NotificationItem>() { @Override public int compare(NotificationItem lhs, NotificationItem rhs) <extra_id_0> }); }', 'inputs': array([    3, 16446,    15,  3517,    56,  4569,  1895,   169,   442,\n",
            "           5,    92,   130,     9,    85,    25,   836,   169,    29,\n",
            "        2808,     8,     7,    52,     5,   201,     9,   312,  2808,\n",
            "          10,    23,     4,  1703,     5,    74,  9719,    25,   836,\n",
            "         169,    29,    16,     7,    19,    27,    12,    35,  1402,\n",
            "           5,   836,   169,  5633,     9,  1895,   169,  3613,     8,\n",
            "       32099,     6,    10,     6,     1], dtype=int32), 'targets_pretokenized': b'{ return rhs.getDate().compareTo(lhs.getDate()); }', 'targets': array([   7,   14, 3613,    4, 5765,   37, 1365,    5, 8804,    4, 5765,\n",
            "         39,    6,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_BLOCK:public Lexicon getLexicon(String lexiconName) { if (! lexiconForName.containsKey(lexiconName)) <extra_id_0> return lexiconForName.get(lexiconName); }', 'inputs': array([    3, 16446,    15,  3517,    56,  4569,     3, 14095,    41,\n",
            "       14095,     5,    31,     3, 23357,    66,     8,     7,    21,\n",
            "         124,     3, 23357,   203,    66,     4,   833,     5, 23357,\n",
            "          66,     8,     8, 32099,    14,     3, 23357,   203,    66,\n",
            "           4,    33,     5, 23357,    66,    10,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'loadLexicon(lexiconName);', 'targets': array([  551, 14095,     5, 23357,    66,    10,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_BLOCK:public void loop() throws InterruptedException { long time = System.nanoTime(); while(run) { while((DELAY = System.nanoTime() - time) < TO_DELAY) <extra_id_0> time = System.nanoTime(); onUpdate(DELAY / 1000000000.f); Thread.sleep((long)(TO_DELAY/10000000)); } }', 'inputs': array([    3, 16446,    15,  3517,    56,  4569,    20,  4059,    16,\n",
            "          42,  1441,    38,     7,   126,   546,    11,   208,     4,\n",
            "        6359,    18,   317,     5,   996,     8,     7,   317,     5,\n",
            "           5,  6900,     2,    11,   208,     4,  6359,    16,   139,\n",
            "         546,     8,   136,     3,  1375,    15,  6900,     2,     8,\n",
            "       32099,   546,    11,   208,     4,  6359,    18, 10475,     5,\n",
            "        6900,     2,   260,  3578,  5067,     4,   165,    10,  1078,\n",
            "           4,  1904,     5,     5,   288,     8,     5,  1375,    15,\n",
            "        6900,     2,  9262,  5067,    79,     6,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'{ Thread.yield(); }', 'targets': array([    7,  1078,     4, 15356,    18,     6,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_BLOCK:public void toggleFingerprintMode(final View view) { if (view.getId() == R.id.create_fingerprint_button) { if (fingerprintCaptureOverlay.isEditModeEnabled() || tracking) <extra_id_0> else { fingerprintCaptureOverlay.enableEditMode(); } showToast(\"edit-mode: \" + fingerprintCaptureOverlay.isEditModeEnabled()); } }', 'inputs': array([    3, 16446,    15,  3517,    56,  4569,    20,  4342, 12273,\n",
            "         270,     5,    64,   886,   403,     8,     7,    21,    17,\n",
            "         712,     4,   428,    16,    40,   544,     4,   111,     4,\n",
            "         160,    15, 20997,    15,  1929,     8,     7,    21,    17,\n",
            "       20997,  3068,  2274,     4,   112, 22986,   456,    16,     3,\n",
            "           2, 14869,     8, 32099,    77,     7,     3, 20997,  3068,\n",
            "        2274,     4,  1990, 22986,    18,     6,     3, 17898,    28,\n",
            "        1769,    90,  1736,    56,    32,    34,     3, 20997,  3068,\n",
            "        2274,     4,   112, 22986,   456,    39,     6,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'{ fingerprintCaptureOverlay.disableEditMode(); }', 'targets': array([    7,     3, 20997,  3068,  2274,     4,  3342, 22986,    18,\n",
            "           6,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_BLOCK:private void filterStream(List<Integer> toShow, List<Integer> toHide) { if (mFiles.isEmpty() || !isSingleFile() || !isSingleMedia() || !AccountUtils.getServerVersion(mAccount).isMediaStreamingSupported()) <extra_id_0> else { toShow.add(R.id.action_stream_media); } }', 'inputs': array([    3, 16446,    15,  3517,    56,  8797,    20,   531,   401,\n",
            "           5,    71,    25,   283,    29,    81,  2590,     9,    85,\n",
            "          25,   283,    29,    81,  3885,     8,     7,    21,    17,\n",
            "          87,   778,     4,   280,    16,     3,     2,   232,   112,\n",
            "        1623,   104,    16,     3,     2,   232,   112,  1623,  1628,\n",
            "          16,     3,     2,   232,   815,   217,     4,    33, 17516,\n",
            "           5, 10683,     8,     4,   112,  1628,  7154,  2170,    60,\n",
            "       32099,    77,     7,    81,  2590,     4,    67,     5,   144,\n",
            "           4,   111,     4,   915,    15,   540,    15,  3451,    10,\n",
            "           6,     6,     1], dtype=int32), 'targets_pretokenized': b'{ toHide.add(R.id.action_stream_media); }', 'targets': array([   7,   81, 3885,    4,   67,    5,  144,    4,  111,    4,  915,\n",
            "         15,  540,   15, 3451,   10,    6,    1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0chadICjOT_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LXgRmevgS6m"
      },
      "source": [
        "### Evaluation\n",
        "You can run the evaluation using the following cells.  \n",
        "Please set the correct path of the variable *MODEL_DIR* (the path to save the pretrained model in)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz1a1TxFNKmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf3132e-b0be-4ac6-f4e3-28e9f2b3135b"
      },
      "source": [
        "def _rate_num_input_examples(task):\n",
        "  if \"train\" in task.splits:\n",
        "    return float(task.num_input_examples(\"train\"))\n",
        "  elif \"validation\" in task.splits:\n",
        "    return float(task.num_input_examples(\"validation\"))\n",
        "  else:\n",
        "    raise ValueError(\"Task %s does not have a train or validation split.\" % (task.name))\n",
        "\n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"all_tasks\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"all_tasks\",\n",
        "    [\"java_construct\", \"java_token\", \"java_block\", \"android_construct\", \"android_token\", \"android_block\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        "     #default_rate=1.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.seqio.dataset_providers.Mixture at 0x7f06e6f5bfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Qx699vN302"
      },
      "source": [
        "from mesh_tensorflow.transformer.learning_rate_schedules import slanted_triangular\n",
        "\n",
        "MODEL_SIZE = \"small\" \n",
        "\n",
        "# Set the folder where the checkpoints and all the others information will be writed\n",
        "MODEL_DIR = 'gs://bucket_code_completion/T5_extension/finetuning'\n",
        "\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    learning_rate_schedule = slanted_triangular,\n",
        "    sequence_length={\"inputs\": 256, \"targets\": 256},\n",
        "    save_checkpoints_steps=5000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyQY1MxTEFUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d87a8342-ef38-470d-dc9c-fec8c721a2cb"
      },
      "source": [
        "# we used model.predict function (setting beam_size)\n",
        "\n",
        "vocabulary_predict=get_default_vocabulary()\n",
        "\n",
        "model.predict(input_file='gs://bucket_code_completion/T5_extension/finetuning/predict/inputs.txt', output_file='gs://bucket_code_completion/T5_extension/finetuning/predict/predictions.txt',\n",
        "              checkpoint_steps=-1, beam_size=1, temperature=0.0, keep_top_k=-1, vocabulary=vocabulary_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://bucket_comment_completion/Matteo/finetuning/operative_config.gin\n",
            "ERROR:root:Path not found: gs://bucket_comment_completion/Matteo/finetuning/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://bucket_comment_completion/Matteo/finetuning', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': graph_options {\n",
            "  rewrite_options {\n",
            "    disable_meta_optimizer: true\n",
            "  }\n",
            "}\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.112.178.66:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.112.178.66:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.112.178.66:8470', '_evaluation_master': 'grpc://10.112.178.66:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f06e6e3b668>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.112.178.66:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Initializing TPU system (master: grpc://10.112.178.66:8470) to fetch topology for model parallelism. This might take a while.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, -630862273210302335)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 7925699374889563133)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -2470163081350250269)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -6359062583073183144)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 7593984121947658758)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 2550645552061072118)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, -6928276260377443211)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 6536614690247464797)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 8835067165760898715)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, -7087564673373198527)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 4999962610807033407)\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:num_cores_per_replica: 1\n",
            "INFO:tensorflow:computation_shape: [1, 1, 1, 1]\n",
            "INFO:tensorflow:num_replicas: 8\n",
            "INFO:tensorflow:device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "INFO:tensorflow:device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_shape=[8] physical_shape=[2, 2, 2]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 0), (1, 0, 1)]\n",
            "WARNING:tensorflow:SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "INFO:tensorflow:SimdMeshImpl init: Shape[batch=8] LayoutRules{('d_ff', 'model'), ('vocab', 'model'), ('experts', 'batch'), ('batch', 'batch'), ('ensemble', 'ensemble'), ('heads', 'model')}\n",
            "INFO:tensorflow:Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7f06d94d88d0>\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:tensorflow:Create pnum_tensor\n",
            "INFO:tensorflow:Casting <dtype: 'int32'> to float32 for allreduce\n",
            "INFO:tensorflow:Casting <dtype: 'int32'> to float32 for allreduce\n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable shared/embedding                                             size 16449536     slice_size 16449536     Shape[vocab=32128, d_model=512]                             \n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/SelfAttention/relative_attention_bias size 512          slice_size 512          Shape[stacked=2, heads=8, buckets=32]                       \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/layer_norm/scale         size 16384        slice_size 16384        Shape[stacked=32, d_model=512]                              \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/final_layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/final_layer_norm/scale\n",
            "INFO:tensorflow:Trainable Variables            count: 99      Total size: 60506624         Total slice_size: 60506624       \n",
            "INFO:tensorflow:All Variables                  count: 99      Total size: 60506624         Total slice_size: 60506624       \n",
            "INFO:tensorflow:Counters:\n",
            "allconcat: 1.05e+06\n",
            " allconcat/0: 1.05e+06\n",
            "  allconcat/0/reshape_op: 1.05e+06\n",
            "allreduce: 8\n",
            " allreduce/[0]: 8\n",
            "  allreduce/[0]/reduce_op: 8\n",
            "einsum: 6.45e+12\n",
            "einsum_unique: 6.45e+12\n",
            "output: 4.61e+10\n",
            " output/AddOperation: 9.11e+09\n",
            " output/BinaryOpWithBroadcasting: 1.15e+08\n",
            " output/Constant: 4.03e+08\n",
            " output/EinsumOperation: 1.29e+10\n",
            " output/ImportOperation: 5.25e+05\n",
            " output/MinMaxOperation: 9.5e+06\n",
            " output/OneHotOperation: 4.41e+09\n",
            " output/RangeOperation: 4.1e+03\n",
            " output/ReduceOperation: 2.1e+07\n",
            " output/ReshapeOperation: 3.46e+09\n",
            " output/ScalarAddOperation: 1.47e+07\n",
            " output/ScalarMultiplyOperation: 1.62e+08\n",
            " output/ShiftOperation: 6.55e+04\n",
            " output/SlicewiseOperation: 1.23e+10\n",
            " output/StackedVariable: 1.35e+05\n",
            " output/StopGradient: 2.42e+09\n",
            " output/UnstackOperation: 1.35e+05\n",
            " output/Variable: 4.84e+08\n",
            " output/WhileLoopOperation: 4.03e+08\n",
            "output_unique: 4.53e+10\n",
            " output_unique/AddOperation: 9.07e+09\n",
            " output_unique/BinaryOpWithBroadcasting: 1.04e+08\n",
            " output_unique/Constant: 4.03e+08\n",
            " output_unique/EinsumOperation: 1.28e+10\n",
            " output_unique/ImportOperation: 6.56e+04\n",
            " output_unique/MinMaxOperation: 1.25e+06\n",
            " output_unique/OneHotOperation: 4.24e+09\n",
            " output_unique/RangeOperation: 512\n",
            " output_unique/ReduceOperation: 2.1e+07\n",
            " output_unique/ReshapeOperation: 3.46e+09\n",
            " output_unique/ScalarAddOperation: 3.67e+06\n",
            " output_unique/ScalarMultiplyOperation: 1.4e+08\n",
            " output_unique/ShiftOperation: 6.55e+04\n",
            " output_unique/SlicewiseOperation: 1.22e+10\n",
            " output_unique/StackedVariable: 1.69e+04\n",
            " output_unique/StopGradient: 2.42e+09\n",
            " output_unique/UnstackOperation: 1.69e+04\n",
            " output_unique/Variable: 6.05e+07\n",
            " output_unique/WhileLoopOperation: 4.03e+08\n",
            "variables: 6.05e+07\n",
            " variables/trainable: 6.05e+07\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://bucket_comment_completion/Matteo/finetuning/model.ckpt-602900\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:840: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Initialized dataset iterators in 1 seconds\n",
            "INFO:tensorflow:Before copy master to slices.\n",
            "INFO:tensorflow:Done with copy master to slices.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:decoded 0: b ⁇ ANDROID_BLOCK:public void onStackEmpty() { cancelAutoLove(); if (progressDialog == null  ⁇  !progressDialog.isShowing()) <extra_id_0> } ⁇ \n",
            "INFO:tensorflow:            -> { progressDialog = new ProgressDialog(this, null, getString(R.string.please_wait)); }\n",
            "INFO:tensorflow:decoded 1: b ⁇ ANDROID_BLOCK:private static List<HeroAndAdvantages> loadHeroes(S ⁇ LiteDatabase db) { List<HeroAndAdvantages> heroes = new ArrayList<>(); Cursor c = db.raw ⁇ uery(\"SELECT  ⁇  FROM Heroes\", null); c.moveToFirst(); while (!c.isAfterLast()) <extra_id_0> c.close(); return heroes; } ⁇ \n",
            "INFO:tensorflow:            -> { heroes.add(loadHeroAndAdvantages(c)); c.moveToNext(); }\n",
            "INFO:tensorflow:decoded 2: b ⁇ ANDROID_BLOCK:private String getLinkByRelation(String relation) { for (Link l : link) { if (l.getRel().equals(relation)) <extra_id_0> } return null; } ⁇ \n",
            "INFO:tensorflow:            -> { return l.getText(); }\n",
            "INFO:tensorflow:decoded 4: b ⁇ ANDROID_BLOCK:public static boolean isBundleValid(final Bundle bundle) { if (null == bundle) { Timber.e(\"bundle is null\"); return false; } if (bundle.getInt(BUNDLE_E ⁇ TRA_INT_VERSION_CODE, -1) == -1) <extra_id_0> if (isNullOrEmpty(bundle.getString(BUNDLE_E ⁇ TRA_STRING_FILTER))) { Timber.e(\"Invalid  ⁇ s\", BUNDLE_E ⁇ TRA_STRING_FILTER); return false; } return true; } ⁇ \n",
            "INFO:tensorflow:            -> { Timber.e(\"Invalid  ⁇ s\", BUNDLE_E ⁇ TRA_INT_VERSION_CODE); return false; }\n",
            "INFO:tensorflow:decoded 8: b ⁇ ANDROID_BLOCK:private void selectUserState(List<Location> locations, Location userLocation) { int userStatePosition = locations.indexOf(userLocation); if(userStatePosition >= 0) <extra_id_0> } ⁇ \n",
            "INFO:tensorflow:            -> { sPosition = userStatePosition; selectUserState(locations.get(userStatePosition)); }\n",
            "INFO:tensorflow:decoded 16: b ⁇ ANDROID_BLOCK:private String getSavedItems() { if (mainListView == null) return \"\"; String savedItems = \"\"; final int count = mainListView.getAdapter().getCount(); for (int i = 0; i < count; i++) { if (mainListView.isItemChecked(i)) { if (savedItems.length() > 0) { savedItems += \",\" + mainListView.getItemAtPosition(i); } else <extra_id_0> } } return savedItems; } ⁇ \n",
            "INFO:tensorflow:            -> { savedItems = mainListView.getItemAtPosition(i); }\n",
            "INFO:tensorflow:decoded 32: b ⁇ ANDROID_BLOCK:public void onClick(View v) { if (mViewPager != null && v instanceof ImageView) { mViewPager.setCurrentItem((Integer) v.getTag()); } if (mClickListener != null) <extra_id_0> } ⁇ \n",
            "INFO:tensorflow:            -> { mClickListener.onClick(v); }\n",
            "INFO:tensorflow:decoded 64: b ⁇ ANDROID_BLOCK:private void connectWifiClient(@NonNull String ip, int port) { if (mWifiClient != null) <extra_id_0> sendSystemMessageToAllLocalClients(EVENT_CONNECTION_STATUS_UPDATE, \"Connecting using Wifi...\"); Log.d(TAG, \"Starting Wifi client connection\"); mWifiClient = new WifiClientConnection(ip, port); new Thread(mWifiClient).start(); setWifiErrorTimeout(); Log.d(TAG, \"Wifi client connection started\"); } ⁇ \n",
            "INFO:tensorflow:            -> { mWifiClient.cancel(); }\n",
            "INFO:tensorflow:decoded 128: b ⁇ ANDROID_BLOCK:public boolean equals(Object o) { if (this == o) { return true; } if (o == null  ⁇  getClass() != o.getClass()) <extra_id_0> OriginalKey that = (OriginalKey) o; return id.equals(that.id) && signature.equals(that.signature); } ⁇ \n",
            "INFO:tensorflow:            -> { return false; }\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:decoded 256: b ⁇ ANDROID_BLOCK:protected void onActivityResult(int requestCode, int resultCode, @Nullable Intent data) { if (requestCode == FILES_ACTIVIT ⁇ _R ⁇  && resultCode == RESULT_OK && data != null && data.hasExtra(SCRIPT_E ⁇ TRA)) <extra_id_0> super.onActivityResult(requestCode, resultCode, data); } ⁇ \n",
            "INFO:tensorflow:            -> { String script = data.getStringExtra(SCRIPT_E ⁇ TRA); script.setScript(script); }\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:decoded 512: b ⁇ ANDROID_BLOCK:public Moment nextMonth() { if (date() != Absolute.unassignedDate) { int month = month(); if (0 != month) { if (12 == month()) { moment -= 11  ⁇  Time.MONTH; next ⁇ ear(); } else <extra_id_0>}} return this; } ⁇ \n",
            "INFO:tensorflow:            -> moment += Time.MONTH;\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:decoded 1024: b ⁇ ANDROID_BLOCK:public int onStartCommand(Intent intent, int flags, int startId) { if (intent.getBooleanExtra(getString(R.string.start_alarm),true)) <extra_id_0> else cancelAlarmNotifications(); return super.onStartCommand(intent, flags, startId); } ⁇ \n",
            "INFO:tensorflow:            -> setAlarmNotifications();\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:decoded 2048: b ⁇ ANDROID_BLOCK:void onCompleteBoxClick(View v) { if (task == null) <extra_id_0> boolean newState = completeBox.isChecked(); if (newState != task.isCompleted()) { taskCompleter.setComplete(task.getTask(), newState); callback.onCompletedTask(task, newState); } setupTitleAndCheckbox(); } ⁇ \n",
            "INFO:tensorflow:            -> { return; }\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:decoded 4096: b ⁇ ANDROID_BLOCK:public void getDataOnOtherThread() throws IOException, RuntimeException { ApiService ⁇ 2Tools api = app.getApiService ⁇ 2Tools(); Response<GroupLarge> ret = api.getFriendsGroups(groupId).execute(); if (Tools.apiIsSuccessful(ret)) group = ret.body(); else <extra_id_0> } ⁇ \n",
            "INFO:tensorflow:            -> throw new RuntimeException(\"The gapi is not successful.\");\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:decoded 8192: b ⁇ ANDROID_BLOCK:public static void updateForecasts(List<Forecast> forecasts) { for(IUpdateableCityUI sub : subscribers) <extra_id_0> } ⁇ \n",
            "INFO:tensorflow:            -> { sub.updateForecasts(forecasts); }\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (60, 0)\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:decoded 16384: b ⁇ ANDROID_BLOCK:private Bitmap crop(BitmapFactory.Options outOptions) throws IOException { InputStream inputStream = openBitmapInputStream(); BitmapRegionDecoder decoder = BitmapRegionDecoder.newInstance(inputStream, false); try <extra_id_0> finally { if (inputStream != null) { inputStream.close(); } decoder.recycle(); } } ⁇ \n",
            "INFO:tensorflow:            -> { return decoder.decode(outOptions); }\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (1) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Enqueue next (1) batch(es) of data to infeed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTr15bwE6YY-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}