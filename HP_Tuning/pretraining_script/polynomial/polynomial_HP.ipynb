{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "polynomial_HP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf2HpC27FCEY"
      },
      "source": [
        "### Configuration of the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h1MRzBLtex2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4effe485-b0b2-48aa-8efd-ea9c7844324c"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip3 install --upgrade pip\n",
        "#!pip install -qU t5\n",
        "!pip3 install git+https://github.com/google-research/text-to-text-transfer-transformer.git #extra_id_x support\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5\n",
        "\n",
        "#Set the base dir(Google cloud bucket)\n",
        "BASE_DIR = \"gs://bucket_code_completion\" \n",
        "\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"2x2\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/ef/60d7ba03b5c442309ef42e7d69959f73aacccd0d86008362a681c4698e83/pip-21.0.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 6.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.0.1\n",
            "Collecting git+https://github.com/google-research/text-to-text-transfer-transformer.git\n",
            "  Cloning https://github.com/google-research/text-to-text-transfer-transformer.git to /tmp/pip-req-build-ntwojsew\n",
            "  Running command git clone -q https://github.com/google-research/text-to-text-transfer-transformer.git /tmp/pip-req-build-ntwojsew\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (0.10.0)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (2.9.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (0.4.0)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "  Downloading mesh_tensorflow-0.1.18-py3-none-any.whl (361 kB)\n",
            "\u001b[K     |████████████████████████████████| 361 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (1.1.5)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 2.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (1.4.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.14 in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (1.15.0)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.4.3-cp36-cp36m-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 20.9 MB/s \n",
            "\u001b[?25hCollecting tfds-nightly\n",
            "  Downloading tfds_nightly-4.2.0.dev202101300107-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 50.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5==0.8.1) (1.7.0+cu101)\n",
            "Collecting transformers>=2.7.0\n",
            "  Downloading transformers-4.2.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 63.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (4.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (4.41.1)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 72.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
            "\u001b[K     |████████████████████████████████| 883 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (0.8)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5==0.8.1) (3.4.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel->t5==0.8.1) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers>=2.7.0->t5==0.8.1) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers>=2.7.0->t5==0.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5==0.8.1) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5==0.8.1) (2.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.7.0->t5==0.8.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.7.0->t5==0.8.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.7.0->t5==0.8.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.7.0->t5==0.8.1) (2.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.2.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5==0.8.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5==0.8.1) (1.0.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (0.1.5)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (3.12.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (0.3.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (20.3.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (0.27.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (5.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (51.3.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5==0.8.1) (1.52.0)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->t5==0.8.1) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->t5==0.8.1) (0.11.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.12.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.36.2)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (2.10.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.32.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.12)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (3.3.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.1.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (4.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow-text->t5==0.8.1) (3.1.0)\n",
            "Building wheels for collected packages: t5, sacremoses\n",
            "  Building wheel for t5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for t5: filename=t5-0.8.1-py3-none-any.whl size=221270 sha256=6288da428449ea97ab0829f40d502b6fa772bd7a4cc922ea474e70efe47ba320\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-97faid98/wheels/aa/e1/a1/847d16e451940b1fe89940aa88875c96ae2f7cc63e509e9226\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=dfe33ac1c05975791133b2f05661025c16b48d2a1cd1f5f56928d2baf2be60fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
            "Successfully built t5 sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, portalocker, mesh-tensorflow, transformers, tfds-nightly, tensorflow-text, sentencepiece, sacrebleu, rouge-score, t5\n",
            "Successfully installed mesh-tensorflow-0.1.18 portalocker-2.2.0 rouge-score-0.0.4 sacrebleu-1.5.0 sacremoses-0.0.43 sentencepiece-0.1.95 t5-0.8.1 tensorflow-text-2.4.3 tfds-nightly-4.2.0.dev202101300107 tokenizers-0.9.4 transformers-4.2.2\n",
            "Running on TPU: grpc://10.74.11.226:8470\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJLX0e7WFQZt"
      },
      "source": [
        "### Loading of the tsv files\n",
        "We loaded the 6 tsv files, please be sure to upload them on the bucket and to copy the correct path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glLJUm1dxIiH"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_java_construct = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_java_construct.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/eval_java_construct.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_java_construct = dict(train=750000, validation=104037)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-B3_th9eP5y"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_android_construct = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_android_construct.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/eval_android_construct.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_android_construct = dict(train=750000, validation=98793)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u7JbyjV8GN3"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_java_block = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_java_block.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/eval_java_block.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_java_block = dict(train=298470, validation=38840)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmUMU-Pg8HVm"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_android_block = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_android_block.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/eval_android_block.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_android_block = dict(train=204580, validation=26503)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5lRmNWG8HuD"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_java_token = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_java_token.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/eval_java_token.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_java_token = dict(train=750000, validation=214682)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF9DHJxc8IGe"
      },
      "source": [
        "#Validation(train and test on the same dataset)\n",
        "\n",
        "nq_tsv_path_android_token = {\n",
        "    \"train\":      'gs://bucket_code_completion/T5_extension/ft_datasets/train_android_token.tsv',\n",
        "    \"validation\": 'gs://bucket_code_completion/T5_extension/ft_datasets/eval_android_token.tsv',\n",
        "}\n",
        "\n",
        "num_nq_examples_android_token = dict(train=750000, validation=198281)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlh8_Fn1FsaP"
      },
      "source": [
        "### Preprocess of the dataset\n",
        "In this step we preprocess the dataset.  \n",
        "You have to change the path to vocab files (*vocab_model_path* and *vocab_path*)\n",
        "We're going to preprocess all the tsv file so that T5 can use them for HP tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PobLvzL18zzR"
      },
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "\n",
        "# # Set the path of sentencepiece model and vocab files\n",
        "# # Must be the same used for the pre-trained phase\n",
        "vocab_model_path = 'gs://bucket_code_completion/T5_extension/code.model'\n",
        "vocab_path = 'gs://bucket_code_completion/T5_extension/code.vocab'\n",
        "\n",
        "\n",
        "TaskRegistry = t5.data.TaskRegistry\n",
        "TfdsTask = t5.data.TfdsTask\n",
        "\n",
        "\n",
        "def get_default_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, 100)\n",
        "\n",
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=True, required=False),\n",
        "\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=True)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn-DMH5FkSO2"
      },
      "source": [
        "JAVA CONSTRUCT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0NTLbyXvkCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417bec9f-6d61-4215-bfe8-0a9c9d4adf0f"
      },
      "source": [
        "def nq_java_construct(split, shuffle_files=True):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "   # Load lines from the text file as examples.\n",
        "\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_java_construct[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw train examples...\")\n",
        "for ex in tfds.as_numpy(nq_java_construct(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw train examples...\n",
            "{'input': b'public void updateLockdownExceptions(ManagedObjectReference _this, String[] users) throws AuthMinimumAdminPermission, RemoteException, RuntimeFault, UserNotFound { Argument[] params = new Argument[2]; params[0] = new Argument(\"_this\", \"ManagedObjectReference\", _this); params[1] = new Argument(\"users\", \"String[]\", users); getWsc().invoke( <extra_id_0>); }', 'output': b'\"UpdateLockdownExceptions\", params, null'}\n",
            "{'input': b'@Override public Collection<AgentProjectInfo> collectDependencies(String folder) { if ( <extra_id_0>){ devDependencies = findDevDependencies(folder); } File yarnLock = new File(folder + fileSeparator + YARN_LOCK); boolean yarnLockFound = yarnLock.isFile(); Collection<DependencyInfo> dependencies = new ArrayList<>(); if (yarnLockFound){ dependencies = parseYarnLock(yarnLock); } else { npmLsFailureStatus = true; } return getSingleProjectList(dependencies); }', 'output': b'!includeDevDependencies'}\n",
            "{'input': b'@Override public Collection<AgentProjectInfo> collectDependencies(String folder) { if (!includeDevDependencies){ devDependencies = findDevDependencies(folder); } File yarnLock = new File(folder + fileSeparator + YARN_LOCK); boolean yarnLockFound = yarnLock.isFile(); Collection<DependencyInfo> dependencies = new ArrayList<>(); if ( <extra_id_0>){ dependencies = parseYarnLock(yarnLock); } else { npmLsFailureStatus = true; } return getSingleProjectList(dependencies); }', 'output': b'yarnLockFound'}\n",
            "{'input': b'@Override public Collection<AgentProjectInfo> collectDependencies(String folder) { if (!includeDevDependencies){ devDependencies = findDevDependencies(folder); } File yarnLock = new File( <extra_id_0>); boolean yarnLockFound = yarnLock.isFile(); Collection<DependencyInfo> dependencies = new ArrayList<>(); if (yarnLockFound){ dependencies = parseYarnLock(yarnLock); } else { npmLsFailureStatus = true; } return getSingleProjectList(dependencies); }', 'output': b'folder + fileSeparator + YARN_LOCK'}\n",
            "{'input': b'@Override public Collection<AgentProjectInfo> collectDependencies(String folder) { if (!includeDevDependencies){ devDependencies = findDevDependencies(folder); } File yarnLock = new File(folder + fileSeparator + YARN_LOCK); boolean yarnLockFound = yarnLock.isFile(); Collection<DependencyInfo> dependencies = new ArrayList<>(); if (yarnLockFound){ dependencies = parseYarnLock(yarnLock); } else { npmLsFailureStatus = true; } return getSingleProjectList( <extra_id_0>); }', 'output': b'dependencies'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bJZPQgjxKZ1"
      },
      "source": [
        "def java_construct_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['JAVA_CONSTRUCT:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3jAg8Zhx_Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5bad30-ee06-4121-eb76-96c896025b89"
      },
      "source": [
        "t5.data.TaskRegistry.remove('java_construct')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"java_construct\",\n",
        "    dataset_fn=nq_java_construct,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[java_construct_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_java_construct\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f6012b0a8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e71p9JIFyYHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23c0bf0-9c5d-44f4-a8b1-a8d858a0ead2"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"java_construct\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'JAVA_CONSTRUCT:public static boolean defaultTransactionSupported(final String persistenceUnit, final KunderaMetadata kunderaMetadata) { PersistenceUnitMetadata puMetadata = KunderaMetadataManager.getPersistenceUnitMetadata(kunderaMetadata, persistenceUnit); String txResource = puMetadata.getProperty(PersistenceProperties.KUNDERA_TRANSACTION_RESOURCE); if ( <extra_id_0>) { return true; } else if (txResource.isEmpty()) { throw new IllegalArgumentException(\"Property \" + PersistenceProperties.KUNDERA_TRANSACTION_RESOURCE + \" is blank\"); } else { return false; } }', 'inputs': array([    3,  7641,    15,  5071, 17558,    56,  4569,    48,    45,\n",
            "         289,   737,  2170,     5,    64,    26,     3, 25534,     9,\n",
            "          44,  2289, 13550,   184,   411,     3, 30065,   411,     8,\n",
            "           7,     3, 27457,   411, 16380,   411,    11,  2289, 13550,\n",
            "         184,   411,   121,     4,    33, 27457,   411,     5, 30065,\n",
            "         411,     9,     3, 25534,    10,    26,  3241,   256,    11,\n",
            "       16380,   411,     4,   761,     5,  2906,   277,     4,   390,\n",
            "       25717,   236,    15, 12610,    15,  4081,    10,    21,    17,\n",
            "       32099,     8,     7,    14,    89,    13,     6,    77,    21,\n",
            "          17,  4674,   256,     4,   280,    60,     7,    78,    24,\n",
            "         381,    38,    28,   220,    32,    34,  7317,   277,     4,\n",
            "         390, 25717,   236,    15, 12610,    15,  4081,    34,    32,\n",
            "          69,  7577,    46,     6,    77,     7,    14,    76,    13,\n",
            "           6,     6,     1], dtype=int32), 'targets_pretokenized': b'txResource == null', 'targets': array([3241,  256,   40,   30,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_CONSTRUCT:public static boolean assertsFirstParameterIsNonnull(XMethod m) { return (\"checkNonNull\".equalsIgnoreCase(m.getName()) || \"checkNotNull\".equalsIgnoreCase(m.getName()) || \"requireNonNull\".equals( <extra_id_0>) || \"isNotNull\".equalsIgnoreCase(m.getName()) || \"assertNotNull\".equalsIgnoreCase(m.getName())) && m.getSignature().startsWith(\"(Ljava/lang/Object;\"); }', 'inputs': array([    3,  7641,    15,  5071, 17558,    56,  4569,    48,    45,\n",
            "         419,    22,  1409,   513,  1045,   790,     5,     2,   268,\n",
            "          54,     8,     7,    14,     3,    28,   847,   747,    83,\n",
            "           4,  1128,     5,    87,     4,   154,    60,     3,     2,\n",
            "          32,  2673,    83,     4,  1128,     5,    87,     4,   154,\n",
            "          60,     3,     2,    32,  2837,    83,     4,   117,     5,\n",
            "       32099,     8,     3,     2,    32,  7586,    83,     4,  1128,\n",
            "           5,    87,     4,   154,    60,     3,     2,    32,  3976,\n",
            "          83,     4,  1128,     5,    87,     4,   154,   459,    91,\n",
            "          54,     4,  8966,    37,  1041,    28,     5,   215,   663,\n",
            "          59,   676,    59,    96,    13,    46,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'm.getName()', 'targets': array([ 54,   4, 154,  16,   1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_CONSTRUCT:public ServerHandle detect(MBeanServerExecutor pMBeanServerExecutor) { Class serverClass = ClassUtil.classForName(\"org.mortbay.jetty.Server\",false); if (serverClass != null) { return new ServerHandle(\"Mortbay\", \"jetty\", getVersion(serverClass), null); } serverClass = ClassUtil.classForName( <extra_id_0>); if (serverClass != null) { return new ServerHandle(\"Eclipse\", \"jetty\", getVersion(serverClass), null); } return null; }', 'inputs': array([    3,  7641,    15,  5071, 17558,    56,  4569,  2352,  1033,\n",
            "        9064,     5,  9205,  1140,   196,  9205,  1140,     8,     7,\n",
            "         335,   563,   114,    11,   335,   306,     4,    88,   203,\n",
            "          66,    28,   372,     4, 28073, 24751,     4, 13609,     4,\n",
            "         344,    43,   348,    10,    21,    17,   907,   114,    49,\n",
            "          30,     8,     7,    14,    24,  2352,  1033,    28,  7312,\n",
            "        5939, 24751,    43,    32, 13609,    43,  4900,     5,   907,\n",
            "         114,     8,     9,    30,    10,     6,   563,   114,    11,\n",
            "         335,   306,     4,    88,   203,    66,     5, 32099,    10,\n",
            "          21,    17,   907,   114,    49,    30,     8,     7,    14,\n",
            "          24,  2352,  1033,    28, 11657,    43,    32, 13609,    43,\n",
            "        4900,     5,   907,   114,     8,     9,    30,    10,     6,\n",
            "          14,    30,    13,     6,     1], dtype=int32), 'targets_pretokenized': b'\"org.eclipse.jetty.server.Server\",false', 'targets': array([   32,   372,     4,  3948,     4, 13609,     4,   907,     4,\n",
            "         344,    43,   348,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_CONSTRUCT:public void removeListener(PBListener listener) throws PersistenceBrokerException { if (listener instanceof PBStateListener) { permanentStateListeners.remove(listener); temporaryStateListeners.remove(listener); } if (listener instanceof PBLifeCycleListener) { permanentLifeCycleListeners.remove( <extra_id_0>); temporaryLifeCycleListeners.remove(listener); } }', 'inputs': array([    3,  7641,    15,  5071, 17558,    56,  4569,    20, 12934,\n",
            "           5,  7564,   222,   499,     8,    42,  7317,  3140,    38,\n",
            "           7,    21,    17,  1252,   166,     3,  7564, 12620,     8,\n",
            "           7, 20237,   119,  1555,     4,   252,     5,  1252,    10,\n",
            "        8621,   119,  1555,     4,   252,     5,  1252,    10,     6,\n",
            "          21,    17,  1252,   166,     3,  7564,  9628,   222,     8,\n",
            "           7, 20237,  9628,  1555,     4,   252,     5, 32099,    10,\n",
            "        8621,  9628,  1555,     4,   252,     5,  1252,    10,     6,\n",
            "           6,     1], dtype=int32), 'targets_pretokenized': b'listener', 'targets': array([499,   1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_CONSTRUCT:public static void printInCaseOfErrorsOrWarnings(Context context, long threshold) { if (context == null) { throw new IllegalArgumentException(\"Context argument cannot be null\"); } StatusManager sm = context.getStatusManager(); if (sm == null) { ps.println( <extra_id_0>); } else { StatusUtil statusUtil = new StatusUtil(context); if (statusUtil.getHighestLevel(threshold) >= ErrorStatus.WARN) { print(sm, threshold); } } }', 'inputs': array([    3,  7641,    15,  5071, 17558,    56,  4569,    48,    20,\n",
            "        1172,   213,  2385,   432,  2280,   686,  8296,     5,    92,\n",
            "         130,     9,   126,  5363,     8,     7,    21,    17,   201,\n",
            "          40,    30,     8,     7,    78,    24,   381,    38,    28,\n",
            "          92,  2208,  1267,   198,    30,    46,     6,     3,   221,\n",
            "         121,  8360,    11,   130,     4,  2172,   121,    18,    21,\n",
            "          17,    22,    87,    40,    30,     8,     7,  4014,     4,\n",
            "         308,     5, 32099,    10,     6,    77,     7,     3,   221,\n",
            "         306,   585,   306,    11,    24,     3,   221,   306,     5,\n",
            "         201,    10,    21,    17,   879,   306,     4, 19238,   377,\n",
            "           5, 10034,     8,   453,  3904,   221,     4,  7897,     8,\n",
            "           7,  1172,     5,    22,    87,     9,  5363,    10,     6,\n",
            "           6,     6,     1], dtype=int32), 'targets_pretokenized': b'\"WARN: Context named \\\\\"\" + context.getName() + \"\\\\\" has no status manager\"', 'targets': array([  32, 7897,   56,    3,   92, 5016,    3,    2, 1812,   34,  130,\n",
            "          4,  154,   16,   34,   32,    2,   83,  358,  650,  585, 1492,\n",
            "         83,    1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB-KY403kcCn"
      },
      "source": [
        "JAVA TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNi7HPiOz27q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c94b8b61-496a-4195-f0ce-dca5a9c967ae"
      },
      "source": [
        "def nq_java_token(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_java_token[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw valid examples...\")\n",
        "for ex in tfds.as_numpy(nq_java_token(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw valid examples...\n",
            "{'input': b'public void addThirdPartyPaymentWorkflow(com.mozu.api.contracts.sitesettings.order.ExternalPaymentWorkflowDefinition definition) throws Exception { MozuClient client = com.mozu.api.clients.commerce.settings.checkout.PaymentSettingsClient <extra_id_0> client.setContext(_apiContext); client.executeRequest(); client.cleanupHttpConnection(); }', 'output': b'.addThirdPartyPaymentWorkflowClient( definition);'}\n",
            "{'input': b'public void addThirdPartyPaymentWorkflow(com.mozu.api.contracts.sitesettings.order.ExternalPaymentWorkflowDefinition definition) throws Exception { MozuClient client = com.mozu.api.clients.commerce.settings.checkout.PaymentSettingsClient.addThirdPartyPaymentWorkflowClient( definition); client.setContext(_apiContext <extra_id_0> client.executeRequest(); client.cleanupHttpConnection(); }', 'output': b');'}\n",
            "{'input': b'public void addThirdPartyPaymentWorkflow(com.mozu.api.contracts.sitesettings.order.ExternalPaymentWorkflowDefinition definition) throws Exception { MozuClient client = com.mozu.api.clients.commerce.settings.checkout.PaymentSettingsClient.addThirdPartyPaymentWorkflowClient( definition); client.setContext(_apiContext); client <extra_id_0> client.cleanupHttpConnection(); }', 'output': b'.executeRequest();'}\n",
            "{'input': b'public void addThirdPartyPaymentWorkflow(com.mozu.api.contracts.sitesettings.order.ExternalPaymentWorkflowDefinition definition) throws Exception { MozuClient client = com.mozu.api.clients.commerce.settings.checkout.PaymentSettingsClient.addThirdPartyPaymentWorkflowClient( definition); client.setContext(_apiContext); client.executeRequest(); client.cleanupHttpConnection <extra_id_0> }', 'output': b'();'}\n",
            "{'input': b'protected Map<String, List<MigratingVariableInstance>> getMigratingVariableInstancesByName(MigratingActivityInstance activityInstance) { Map<String, List<MigratingVariableInstance>> result = new HashMap<String, List<MigratingVariableInstance <extra_id_0> for (MigratingInstance migratingInstance : activityInstance.getMigratingDependentInstances()) { if (migratingInstance instanceof MigratingVariableInstance) { MigratingVariableInstance migratingVariableInstance = (MigratingVariableInstance) migratingInstance; CollectionUtil.addToMapOfLists(result, migratingVariableInstance.getVariableName(), migratingVariableInstance); } } return result; }', 'output': b'>>();'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvDAbgNY0B4Y"
      },
      "source": [
        "def java_token_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['JAVA_TOKEN:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mm6AQfw0INC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a8011cd-aabd-49a0-fca2-ca3e51691cb1"
      },
      "source": [
        "t5.data.TaskRegistry.remove('java_token')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"java_token\",\n",
        "    dataset_fn=nq_java_token,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[java_token_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_java_token\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f6012a95cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnf25qt10Wkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b1cbbf-8246-4a64-d58e-7ff2f2156d83"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"java_token\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'JAVA_TOKEN:private List<ValidationError> validate(ExecutionInput executionInput, Document <extra_id_0> InstrumentationContext<List<ValidationError>> validationCtx = instrumentation.beginValidation(new InstrumentationValidationParameters(executionInput, document, graphQLSchema, instrumentationState)); Validator validator = new Validator(); List<ValidationError> validationErrors = validator.validateDocument(graphQLSchema, document); validationCtx.onCompleted(validationErrors, null); return validationErrors; }', 'inputs': array([    3,  7641,    15,  2591,    56,  8797,    85,    25,  8927,\n",
            "          29,   858,     5,  1184,   488,  2717,   488,     9,  1168,\n",
            "       32099, 13539,    92,    25,    71,    25,  8927,   243,  3756,\n",
            "        4096,    11, 18230,     4,  3090,  1505,     5,    74, 13539,\n",
            "        1505,   542,     5,  4713,   488,     9,   983,     9,  1985,\n",
            "           2,   215,   645,     9, 18230,   119,    79,  9099,  3183,\n",
            "          11,    24,  9099,    18,    85,    25,  8927,    29,     3,\n",
            "       20330,    11,  3183,     4,  1698,   464,     5,  2649,     2,\n",
            "         215,   645,     9,   983,    10,  3756,  4096,     4, 16392,\n",
            "           5, 20330,     9,    30,    10,    14,     3, 20330,    13,\n",
            "           6,     1], dtype=int32), 'targets_pretokenized': b'document, GraphQLSchema graphQLSchema, InstrumentationState instrumentationState) {', 'targets': array([  983,     9,  5975,     2,   215,   645,  1985,     2,   215,\n",
            "         645,     9, 13539,   119, 18230,   119,     8,     7,     1],\n",
            "      dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_TOKEN:@Override public int pHashinateBytes(byte[] bytes) { ByteBuffer buf = ByteBuffer.wrap(bytes); final int token = MurmurHash3.hash3_x64_128(buf, 0, bytes.length, <extra_id_0> return partitionForToken(token); }', 'inputs': array([    3,  7641,    15,  2591,    56,  2098,    27,    12,    35,\n",
            "         196,  1198,   321,  5460,   571,     5,   379,    61,   860,\n",
            "           8,     7,  1530,   597,    11,  1530,     4,  2404,     5,\n",
            "        1216,    10,    44,    35,   782,    11,  1653, 31850,  1198,\n",
            "           2,     4,  1748,     2,    15,   138,     2, 18660,     2,\n",
            "           5,  1385,     9,   312,   860,     4,   105,     9, 32099,\n",
            "          14,  2617,   203,   338,     5,  1021,    10,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'0);', 'targets': array([522,   1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_TOKEN:public static RuntimeException wrap(String loc, String msg, Throwable e) { if (e instanceof UserMessageLocation) { if (e instanceof RuntimeException) { return (RuntimeException) e; } else { return new ConfigExceptionLocation(msg, e); } } else { return new ConfigExceptionLocation(loc + msg <extra_id_0> } }', 'inputs': array([    3,  7641,    15,  2591,    56,  4569,    48,     3,   468,\n",
            "        3347,     5,    31,  2603,     9,    26,   454,     9,   668,\n",
            "          57,     8,     7,    21,    17,   110,   166,   965,   155,\n",
            "         331,     8,     7,    21,    17,   110,   166,     3,   468,\n",
            "           8,     7,    14,    17,   468,     8,    57,    13,     6,\n",
            "          77,     7,    14,    24, 22651,   331,     5,   744,     9,\n",
            "          57,    10,     6,     6,    77,     7,    14,    24, 22651,\n",
            "         331,     5,  2682,    34,   454, 32099,     6,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b', e);', 'targets': array([ 3,  9, 57, 10,  1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_TOKEN:@Override public void execute(TestReport report, Browser browser, GalenPageTest pageTest, ValidationListener validationListener <extra_id_0> MutationReport mutationReport = GalenMutate.checkAllMutations(browser, specPath, includedTags, excludedTags, mutationOptions, getCurrentProperties(), validationListener); if (mutationReport.getInitialLayoutReport() != null) { GalenUtils.attachLayoutReport(mutationReport.getInitialLayoutReport(), report, specPath, includedTags); } GalenUtils.attachMutationReport(mutationReport, report, specPath, includedTags); }', 'inputs': array([    3,  7641,    15,  2591,    56,  2098,    27,    12,    20,\n",
            "         768,     5, 22630,  1292,     9, 10447,  4106,     9,     3,\n",
            "       19995,  1212,   410,   231,   715,   231,     9,  4107,   222,\n",
            "        3756,   222, 32099, 15828,   818, 10585,   818,    11,     3,\n",
            "       19995,  1212, 23463,     4,   847,   517, 22955,     5,  7090,\n",
            "           9,  2620,   128,     9,  8383,  1478,     9,  9362,  1478,\n",
            "           9, 10585,   369,     9,  1873,   277,    72,  3756,   222,\n",
            "          10,    21,    17, 20930,   818,     4, 19573,   673,   818,\n",
            "          16,    49,    30,     8,     7,     3, 19995,  1212,   217,\n",
            "           4,  9447,   673,   818,     5, 20930,   818,     4, 19573,\n",
            "         673,   818,    72,  1292,     9,  2620,   128,     9,  8383,\n",
            "        1478,    10,     6,     3, 19995,  1212,   217,     4,  9447,\n",
            "        7429,   818,     5, 20930,   818,     9,  1292,     9,  2620,\n",
            "         128,     9,  8383,  1478,    10,     6,     1], dtype=int32), 'targets_pretokenized': b') throws Exception {', 'targets': array([  3,   8,  42, 172,   7,   1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_TOKEN:public static void write(OutputStream os, String s) throws <extra_id_0> { int len = s.length(); for (int i = 0; i < len; i++) { write(os, s.charAt(i)); } }', 'inputs': array([    3,  7641,    15,  2591,    56,  4569,    48,    20,   412,\n",
            "           5,   727,  2397,     9,    26,     3,    22,     8,    42,\n",
            "       32099,     7,    35,  1035,    11,     3,    22,     4,   105,\n",
            "          18,    50,    17,    53,    65,    11,   116,    65,   136,\n",
            "        1035,    13,    65,   227,     7,   412,     5,  1286,     9,\n",
            "           3,    22,     4,  1586,     5,    86,    79,     6,     6,\n",
            "           1], dtype=int32), 'targets_pretokenized': b'IOException', 'targets': array([115,   1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIe-u5l9ke6x"
      },
      "source": [
        "JAVA BLOCK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr0TT18ejMtY",
        "outputId": "a48ea037-0a89-43d7-e206-4233a6f381e5"
      },
      "source": [
        "def nq_java_block(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_java_block[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw valid examples...\")\n",
        "for ex in tfds.as_numpy(nq_java_block(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw valid examples...\n",
            "{'input': b'public Object createField(String name) { Object reader = _fieldMap.get(name); if (reader == null) <extra_id_0> return reader; }', 'output': b'reader = NullFieldDeserializer.DESER;'}\n",
            "{'input': b'@Nonnull public static JSInvocation invoke (@Nonnull final JQueryInvocation aJQueryInvocation, @Nonnull final JSAssocArray aOptions) <extra_id_0>', 'output': b'{ return invoke (aJQueryInvocation).arg (aOptions); }'}\n",
            "{'input': b'public static String presentMinMaxCount(long minmax) { if (minmax == Long.MAX_VALUE || minmax == Long.MIN_VALUE) <extra_id_0> return String.valueOf(minmax); }', 'output': b'{ return UNDEF_STRING; }'}\n",
            "{'input': b'@Override public NonBottomTypeNode<ElkClass, ElkNamedIndividual> getCreateNode( final Collection<? extends ElkClass> members) <extra_id_0>', 'output': b'{ return getCreateUpdateableTypeNode( classTaxonomy_.getCreateNode(members)); }'}\n",
            "{'input': b'public static JsonException typeMismatch(Object indexOrName, Object actual, String requiredType, boolean mode) throws JsonException { if (actual == null) <extra_id_0> else { throw new JsonException(\"Value \" + actual + \" at \" + indexOrName + \" of type \" + actual.getClass().getName() + \" cannot be converted to \" + requiredType + \". Strict mode is: \" + mode); } }', 'output': b'{ throw new JsonException(\"Value at \" + indexOrName + \" is null.\"); }'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq0uLYNTjM9z"
      },
      "source": [
        "def java_block_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['JAVA_BLOCK:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji4u8yhqjNER",
        "outputId": "08b183b3-e0d7-4656-db07-59072c02c115"
      },
      "source": [
        "t5.data.TaskRegistry.remove('java_block')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"java_block\",\n",
        "    dataset_fn=nq_java_block,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[java_block_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_java_block\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f6012b0a978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG09lDZdjNKr",
        "outputId": "8ee07d76-d982-4638-d3e4-56e5ef78fdf6"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"java_block\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'JAVA_BLOCK:public List<FileMetaData> getFiles(int level) { if (level == 0) { return level0.getFiles(); } else <extra_id_0> }', 'inputs': array([    3,  7641,    15,  3517,    56,  4569,    85,    25,   104,\n",
            "        1598,    29, 19493,     5,    53,   694,     8,     7,    21,\n",
            "          17,  1544,    40,   178,     7,    14,   694,   148,     4,\n",
            "          33,   778,    18,     6,    77, 32099,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'{ return levels.get(level - 1).getFiles(); }', 'targets': array([    7,    14, 15313,     4,    33,     5,  1544,   139,   637,\n",
            "           4,    33,   778,    18,     6,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_BLOCK:@Override protected boolean isValueExpectedRecursive(XExpression expr) { final EObject container = expr.eContainer(); if (container instanceof SarlBreakExpression) <extra_id_0> if (container instanceof SarlContinueExpression) { return false; } if (container instanceof SarlAssertExpression) { return true; } return super.isValueExpectedRecursive(expr); }', 'inputs': array([    3,  7641,    15,  3517,    56,  2098,    27,    73,    45,\n",
            "          69,   106,  1783,  7255,     5,     2,   378,  2416,     8,\n",
            "           7,    44,  8021,   837,    11,  2416,     4,   110,   402,\n",
            "          18,    21,    17,  1446,   166,   275,  1535,   526,  3932,\n",
            "         378,     8, 32099,    21,    17,  1446,   166,   275,  1535,\n",
            "         526,  6987,   378,     8,     7,    14,    76,    13,     6,\n",
            "          21,    17,  1446,   166,   275,  1535,   526,  2179,   378,\n",
            "           8,     7,    14,    89,    13,     6,    14,    52,     4,\n",
            "         112,   106,  1783,  7255,     5,  3099,    10,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'{ return false; }', 'targets': array([ 7, 14, 76, 13,  6,  1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_BLOCK:@Override public ObjectStore getObjectStore() { if (isActive()) <extra_id_0> throw new RuntimeException(\"Context is not active: \" + super.getClass().getSimpleName()); }', 'inputs': array([    3,  7641,    15,  3517,    56,  2098,    27,    12,   102,\n",
            "         475,  5415,   475,    16,     7,    21,    17,  5649,    60,\n",
            "       32099,    78,    24,     3,   468,    28,    92,    69,   153,\n",
            "        1554,    56,    32,    34,    52,     4,   398,    37,  1673,\n",
            "          39,     6,     1], dtype=int32), 'targets_pretokenized': b'{ return activeStore.get().peek().getStore(); }', 'targets': array([   7,   14, 1554,  475,    4,   33,   37, 3333,   37,   33,  475,\n",
            "         18,    6,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_BLOCK:public static String currentUserName() { Config config = parseConfigs(); if (config != null) { Context context = getCurrentContext(config); if (context != null) { String user = context.getUser(); if (user != null) { String[] parts = user.split(\"/\"); if (parts.length > 0) <extra_id_0> return user; } } } return null; }', 'inputs': array([    3,  7641,    15,  3517,    56,  4569,    48,    26,  8640,\n",
            "          66,    16,     7,     3,   151,   309,    11,   545,  5197,\n",
            "          18,    21,    17,   505,    49,    30,     8,     7,     3,\n",
            "          92,   130,    11,  1873,    92,     5,   505,    10,    21,\n",
            "          17,   201,    49,    30,     8,     7,    26,   269,    11,\n",
            "         130,     4,  2159,    18,    21,    17,   418,    49,    30,\n",
            "           8,     7,    26,    61,  2868,    11,   269,     4,   850,\n",
            "        1100,    46,    21,    17,  4193,     4,   105,     3,    29,\n",
            "         178, 32099,    14,   269,    13,     6,     6,     6,    14,\n",
            "          30,    13,     6,     1], dtype=int32), 'targets_pretokenized': b'{ return parts[0]; }', 'targets': array([   7,   14, 2868, 1659,    6,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'JAVA_BLOCK:private URL createURL(final String urlString) throws MalformedURLException { URL url; try { url = AccessController.doPrivileged(new PrivilegedExceptionAction<URL>() { @Override public URL run() throws MalformedURLException { return new URL(urlString); } }); } catch (PrivilegedActionException e) <extra_id_0> return url; }', 'inputs': array([    3,  7641,    15,  3517,    56,  8797,  1080,   131,   420,\n",
            "           5,    64,    26,   427,    31,     8,    42,     3,  4277,\n",
            "          38,     7,  1080,   427,    13,    93,     7,   427,    11,\n",
            "           3, 25603,     4, 29556,     5,    74,     3, 20152,    38,\n",
            "         271,    25,   420,    29,    16,     7,    19,    27,    12,\n",
            "        1080,   333,    16,    42,     3,  4277,    38,     7,    14,\n",
            "          24,  1080,     5,   569,    31,    10,     6,     6,    10,\n",
            "           6,    97,    17, 20152, 15412,    57,     8, 32099,    14,\n",
            "         427,    13,     6,     1], dtype=int32), 'targets_pretokenized': b'{ throw (MalformedURLException) e.getCause(); }', 'targets': array([   7,   78,   17, 4277,   38,    8,   57,    4, 3804,   18,    6,\n",
            "          1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz_Gxq_4khQt"
      },
      "source": [
        "ANDROID CONSTRUCT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNwnQAMVjNdy",
        "outputId": "b1ad5a6e-2e5a-4745-8833-928fa94016e9"
      },
      "source": [
        "def nq_android_construct(split, shuffle_files=True):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "   # Load lines from the text file as examples.\n",
        "\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_android_construct[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw train examples...\")\n",
        "for ex in tfds.as_numpy(nq_android_construct(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw train examples...\n",
            "{'input': b'private void writeToFile(final String content) { if ( <extra_id_0>) { return; } try { System.err.println(content); File f = new File(OUTPUT_FILE); f.delete(); FileWriter fstream = new FileWriter(OUTPUT_FILE); BufferedWriter out = new BufferedWriter(fstream); out.write(content); out.close(); } catch (Exception e) { e.printStackTrace(); } }', 'output': b'!WRITE_TO_FILE'}\n",
            "{'input': b'private void writeToFile(final String content) { if (!WRITE_TO_FILE) { return; } try { System.err.println(content); File f = new File(OUTPUT_FILE); f.delete(); FileWriter fstream = new FileWriter(OUTPUT_FILE); BufferedWriter out = new BufferedWriter(fstream); out.write(content); out.close(); } catch ( <extra_id_0>) { e.printStackTrace(); } }', 'output': b'Exception e'}\n",
            "{'input': b'private void writeToFile(final String content) { if (!WRITE_TO_FILE) { return; } try { System.err.println(content); File f = new File( <extra_id_0>); f.delete(); FileWriter fstream = new FileWriter(OUTPUT_FILE); BufferedWriter out = new BufferedWriter(fstream); out.write(content); out.close(); } catch (Exception e) { e.printStackTrace(); } }', 'output': b'OUTPUT_FILE'}\n",
            "{'input': b'private void writeToFile(final String content) { if (!WRITE_TO_FILE) { return; } try { System.err.println( <extra_id_0>); File f = new File(OUTPUT_FILE); f.delete(); FileWriter fstream = new FileWriter(OUTPUT_FILE); BufferedWriter out = new BufferedWriter(fstream); out.write(content); out.close(); } catch (Exception e) { e.printStackTrace(); } }', 'output': b'content'}\n",
            "{'input': b'private void writeToFile(final String content) { if (!WRITE_TO_FILE) { return; } try { System.err.println(content); File f = new File(OUTPUT_FILE); f.delete(); FileWriter fstream = new FileWriter(OUTPUT_FILE); BufferedWriter out = new BufferedWriter(fstream); out.write( <extra_id_0>); out.close(); } catch (Exception e) { e.printStackTrace(); } }', 'output': b'content'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sBEViP5jNja"
      },
      "source": [
        "def android_construct_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['ANDROID_CONSTRUCT:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP_VXjC6jNpp",
        "outputId": "ccbd6d5f-31a5-4f32-f1c2-159e089682e5"
      },
      "source": [
        "t5.data.TaskRegistry.remove('android_construct')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"android_construct\",\n",
        "    dataset_fn=nq_android_construct,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[android_construct_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_android_construct\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f601295ed30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOWFYL7KjNwd",
        "outputId": "21fb8fd1-2d90-4625-b7d3-a9b64def5672"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"android_construct\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'ANDROID_CONSTRUCT:public AltosMag(AltosLink link) throws InterruptedException, TimeoutException { this(); link.printf(\"M\\\\ \"); for (;;) { String line = link.get_reply_no_dialog(5000); if ( <extra_id_0>) { throw new TimeoutException(); } if (parse_string(line)) break; } }', 'inputs': array([    3, 16446,    15,  5071, 17558,    56,  4569,     3,  5134,\n",
            "        1286, 29652,     5,  5134,  1286,   651,  1548,     8,    42,\n",
            "        1441,    38,     9,     3,  6702,     7,    23,    18,  1548,\n",
            "           4,  7103,    28,   491,     2,     3,    46,    50,    17,\n",
            "          13,    13,     8,     7,    26,   626,    11,  1548,     4,\n",
            "          33,    15,  7911,    15,  1137,    15,  2472,     5,     2,\n",
            "        8132,    21,    17, 32099,     8,     7,    78,    24,     3,\n",
            "        6702,    18,     6,    21,    17,   655,    15,   383,     5,\n",
            "         687,     8,     8,   591,    13,     6,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'line == null', 'targets': array([626,  40,  30,   1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_CONSTRUCT:public int onStartCommand(Intent intent, int flags, int startId) { if(intent != null) { String action = intent.getAction(); if(TOGGLE_ACTION.equals( <extra_id_0>)) { toggleTorch(); } } return super.onStartCommand(intent, flags, startId); }', 'inputs': array([    3, 16446,    15,  5071, 17558,    56,  4569,    35, 28462,\n",
            "           5,   527,   576,     9,    35,  1936,     9,    35,   241,\n",
            "          68,     8,     7,    21,     5,  1482,    49,    30,     8,\n",
            "           7,    26,   647,    11,   576,     4,  2677,    18,    21,\n",
            "           5, 24862,    15,  1023,     4,   117,     5, 32099,     8,\n",
            "           8,     7,  4342, 20666,    18,     6,     6,    14,    52,\n",
            "           4,  8435,   304,     5,  1482,     9,  1936,     9,   241,\n",
            "          68,    10,     6,     1], dtype=int32), 'targets_pretokenized': b'action', 'targets': array([647,   1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_CONSTRUCT:public String loadSyncFilesList() { String names = null; try { names = loadFileToString(syncFilesListFile); } catch ( <extra_id_0>) { e.printStackTrace(); } return names; }', 'inputs': array([    3, 16446,    15,  5071, 17558,    56,  4569,    26,   551,\n",
            "        1225,   778,    71,    16,     7,    26,  1886,    11,    30,\n",
            "          13,    93,     7,  1886,    11,   551,   104,  1978,     5,\n",
            "        2855,   778,    71,   104,    10,     6,    97,    17, 32099,\n",
            "           8,     7,    57,     4,   864,    18,     6,    14,  1886,\n",
            "          13,     6,     1], dtype=int32), 'targets_pretokenized': b'IOException e', 'targets': array([115,  57,   1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_CONSTRUCT:private void startImageCropActivity(Uri srcUri) { final Activity activity = getActivity(); if (activity == null) { return; } Crop.of( <extra_id_0>).withMaxSize(MAX_AVATAR_SIZE_PIXELS, MAX_AVATAR_SIZE_PIXELS).start(activity); }', 'inputs': array([    3, 16446,    15,  5071, 17558,    56,  8797,    20,   241,\n",
            "         516,  4581,   435,     5,   362,   960,   362,     8,     7,\n",
            "          44,  3203,  1028,    11,  3027,    18,    21,    17,  1338,\n",
            "          40,    30,     8,     7,    14,    13,     6,     3,  4581,\n",
            "           4,   579,     5, 32099,     8,     4,   616, 12015,     5,\n",
            "         887,     2,    15,  7285,   236, 23844,    15,   991,     2,\n",
            "         146,    15,  3076,     2,  5122,   113,     9,  3249,     2,\n",
            "          15,  7285,   236, 23844,    15,   991,     2,   146,    15,\n",
            "        3076,     2,  5122,   113,     8,     4,   373,     5,  1338,\n",
            "          10,     6,     1], dtype=int32), 'targets_pretokenized': b'srcUri, mNewAvatarImageUri', 'targets': array([  960,   362,     9, 18195,  7470,   516,   362,     1],\n",
            "      dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_CONSTRUCT:public static String utf8HexEncode(String s) { if ( <extra_id_0>) { return null; } byte[] utf8; try { utf8 = s.getBytes(Constants.UTF_8); } catch (UnsupportedEncodingException x) { throw new RuntimeException(x); } return hexEncode(utf8); }', 'inputs': array([    3, 16446,    15,  5071, 17558,    56,  4569,    48,    26,\n",
            "           3,  6164,     2,  3617,  5396,     5,    31,     3,    22,\n",
            "           8,     7,    21,    17, 32099,     8,     7,    14,    30,\n",
            "          13,     6,   210,    61,     3,  6164,     2,    13,    93,\n",
            "           7,     3,  6164,     2,    11,     3,    22,     4,  1322,\n",
            "           5,   357,     4,  1227,    15,     2,    10,     6,    97,\n",
            "          17,  5912,  4128,   205,     8,     7,    78,    24,     3,\n",
            "         468,     5,   138,    10,     6,    14,  5728,  5396,     5,\n",
            "        6164,     2,    10,     6,     1], dtype=int32), 'targets_pretokenized': b's == null', 'targets': array([ 3, 22, 40, 30,  1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ9H1D-KngJy"
      },
      "source": [
        "ANDROID TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgi_2yd-nb4Y",
        "outputId": "4475f2d5-1638-43d3-d752-463308ced412"
      },
      "source": [
        "def nq_android_token(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_android_token[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw valid examples...\")\n",
        "for ex in tfds.as_numpy(nq_android_token(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw valid examples...\n",
            "{'input': b'public void register(Context context <extra_id_0> IntentFilter intentFilter = buildFilter(); try { register(context, intentFilter); }catch (IllegalArgumentException e){ Log.w(TAG, \"Error registering receiver. Receiver maybe already registered, unregistering to register again.\"); unregister(context); register(context, intentFilter); } }', 'output': b') {'}\n",
            "{'input': b'public void register(Context context) { IntentFilter intentFilter <extra_id_0> try { register(context, intentFilter); }catch (IllegalArgumentException e){ Log.w(TAG, \"Error registering receiver. Receiver maybe already registered, unregistering to register again.\"); unregister(context); register(context, intentFilter); } }', 'output': b'= buildFilter();'}\n",
            "{'input': b'public void register(Context context) { IntentFilter intentFilter = buildFilter(); try <extra_id_0> register(context, intentFilter); }catch (IllegalArgumentException e){ Log.w(TAG, \"Error registering receiver. Receiver maybe already registered, unregistering to register again.\"); unregister(context); register(context, intentFilter); } }', 'output': b'{'}\n",
            "{'input': b'public void register(Context context) { IntentFilter intentFilter = buildFilter(); try { register(context, intentFilter <extra_id_0> }catch (IllegalArgumentException e){ Log.w(TAG, \"Error registering receiver. Receiver maybe already registered, unregistering to register again.\"); unregister(context); register(context, intentFilter); } }', 'output': b');'}\n",
            "{'input': b'public void register(Context context) { IntentFilter intentFilter = buildFilter(); try { register(context, intentFilter); }catch (IllegalArgumentException <extra_id_0> Log.w(TAG, \"Error registering receiver. Receiver maybe already registered, unregistering to register again.\"); unregister(context); register(context, intentFilter); } }', 'output': b'e){'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcKSDpJrnb_X"
      },
      "source": [
        "def android_token_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['ANDROID_TOKEN:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvwEpdZIjN_9",
        "outputId": "9fbacfe4-3f19-4b18-8579-10570d7a308f"
      },
      "source": [
        "t5.data.TaskRegistry.remove('android_token')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"android_token\",\n",
        "    dataset_fn=nq_android_token,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[android_token_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_android_token\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f60128e4f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6z53ZuUpT3f",
        "outputId": "93375152-15c4-4a55-f30d-59163fbb55c7"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"android_token\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'ANDROID_TOKEN:public void switchToScreen(final <extra_id_0> Validate.notNull(newScreen); final MyScreen screen = this.instantiateScreen(newScreen); assert (!(screen instanceof ParameterScreen)); this.switchControl(screen); }', 'inputs': array([    3, 16446,    15,  2591,    56,  4569,    20,     3, 16074,\n",
            "        1260,     5,    64, 32099,  3109,     4,  2623,     5,    74,\n",
            "        1260,    10,    44,  2945,  1260,  2230,    11,    23,     4,\n",
            "       16170,  1260,     5,    74,  1260,    10,   419,   124,     5,\n",
            "        3339,   166,  4414,  1260,    79,    23,     4,  6916,   863,\n",
            "           5,  3339,    10,     6,     1], dtype=int32), 'targets_pretokenized': b'Screens newScreen) {', 'targets': array([   3, 1260,   22,   24, 1260,    8,    7,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_TOKEN:public GoogleAnalytics provideGoogleAnalytics(Application application) { GoogleAnalytics ga = GoogleAnalytics.getInstance(application); ga.enableAutoActivityReports(application); ga.enableAdvertisingIdCollection <extra_id_0> return ga; }', 'inputs': array([    3, 16446,    15,  2591,    56,  4569,     3, 31569,  4820,\n",
            "       31569,     5,   583,  1344,     8,     7,     3, 31569,     3,\n",
            "        6319,    11,     3, 31569,     4,   351,     5,  1317,    10,\n",
            "           3,  6319,     4,  1990,  1945,   435,  6960,     5,  1317,\n",
            "          10,     3,  6319,     4,  1990, 16502,    68,   387, 32099,\n",
            "          14,     3,  6319,    13,     6,     1], dtype=int32), 'targets_pretokenized': b'(true);', 'targets': array([ 17, 225,  10,   1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_TOKEN:private <extra_id_0> File downloads = Environment .getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS); try { downloads.mkdirs(); } catch (Exception ex) { } fileName = downloads.getAbsolutePath() + \"/\" + item.getName(); return fileName; }', 'inputs': array([    3, 16446,    15,  2591,    56,  8797, 32099,   181,  2237,\n",
            "          22,    11,  6872,     3,     4,    33, 16407,  3277,   662,\n",
            "           5,  1176,     4,  6441,     2,    15,  9798,   113,    10,\n",
            "          93,     7,  2237,    22,     4,  4187,    18,     6,    97,\n",
            "          17,    38,   480,     8,     7,     6,  1299,    11,  2237,\n",
            "          22,     4,  1543,    16,    34,  1573,    34,   323,     4,\n",
            "         154,    18,    14,  1299,    13,     6,     1], dtype=int32), 'targets_pretokenized': b'String getFilename(FileItem item) {', 'targets': array([   26,     3, 11548,     5, 13492,   323,     8,     7,     1],\n",
            "      dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_TOKEN:public HistogramStretching(Bitmap bitmap) { super(bitmap); this.originalBitmap <extra_id_0> this.min = 0; this.max = 255; this.low = new int[3]; this.high = new int[3]; this.ratio = new float[3]; setScalingCoefficients(); balanceWhite(); }', 'inputs': array([    3, 16446,    15,  2591,    56,  4569, 15843, 26233,   150,\n",
            "           5,  1844,  2938,     8,     7,    52,     5,  4539,    10,\n",
            "          23,     4,  2324,  1844, 32099,    23,     4,   769,    11,\n",
            "         116,    23,     4,   532,    11,   804,     2,    13,    23,\n",
            "           4,  6135,    11,    24,    35,    95,     2,   354,    23,\n",
            "           4,  7430,    11,    24,    35,    95,     2,   354,    23,\n",
            "           4, 15236,    11,    24,   245,    95,     2,   354,    55,\n",
            "       11537, 15882,    22,    18,  9637, 12445,    18,     6,     1],\n",
            "      dtype=int32), 'targets_pretokenized': b'= bitmap;', 'targets': array([  11, 2938,   13,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_TOKEN:public boolean fileExists() { if (file_url == null) { return false; } <extra_id_0> File f = new File(file_url); return f.exists(); } }', 'inputs': array([    3, 16446,    15,  2591,    56,  4569,    45,   191,  1841,\n",
            "          16,     7,    21,    17,   296,    15,   569,    40,    30,\n",
            "           8,     7,    14,    76,    13,     6, 32099,   181,   328,\n",
            "          11,    24,   181,     5,   296,    15,   569,    10,    14,\n",
            "         328,     4,  1090,    18,     6,     6,     1], dtype=int32), 'targets_pretokenized': b'else {', 'targets': array([77,  7,  1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q3xYhUwoBC6"
      },
      "source": [
        "ANDROID BLOCK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqvJtXAKjOFn",
        "outputId": "5fcb81ea-d40c-4a57-ba39-00fc139a4c78"
      },
      "source": [
        "def nq_android_block(split, shuffle_files=False):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path_android_block[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "print(\"A few raw valid examples...\")\n",
        "for ex in tfds.as_numpy(nq_android_block(\"validation\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A few raw valid examples...\n",
            "{'input': b'public void update() { if(Gdx.input.isKeyPressed(Keys.LEFT))<extra_id_0> if(Gdx.input.isKeyPressed(Keys.RIGHT)){ bucket.x += 200 * Gdx.graphics.getDeltaTime(); } if (bucket.x < 0) bucket.x = 0; if (bucket.x > 320 - 64) bucket.x = 320 - 64; }', 'output': b'{ bucket.x -= 200 * Gdx.graphics.getDeltaTime(); }'}\n",
            "{'input': b'public void update() { if(Gdx.input.isKeyPressed(Keys.LEFT)){ bucket.x -= 200 * Gdx.graphics.getDeltaTime(); } if(Gdx.input.isKeyPressed(Keys.RIGHT))<extra_id_0> if (bucket.x < 0) bucket.x = 0; if (bucket.x > 320 - 64) bucket.x = 320 - 64; }', 'output': b'{ bucket.x += 200 * Gdx.graphics.getDeltaTime(); }'}\n",
            "{'input': b'public void update() { if(Gdx.input.isKeyPressed(Keys.LEFT)){ bucket.x -= 200 * Gdx.graphics.getDeltaTime(); } if(Gdx.input.isKeyPressed(Keys.RIGHT)){ bucket.x += 200 * Gdx.graphics.getDeltaTime(); } if (bucket.x < 0) <extra_id_0> if (bucket.x > 320 - 64) bucket.x = 320 - 64; }', 'output': b'bucket.x = 0;'}\n",
            "{'input': b'public void update() { if(Gdx.input.isKeyPressed(Keys.LEFT)){ bucket.x -= 200 * Gdx.graphics.getDeltaTime(); } if(Gdx.input.isKeyPressed(Keys.RIGHT)){ bucket.x += 200 * Gdx.graphics.getDeltaTime(); } if (bucket.x < 0) bucket.x = 0; if (bucket.x > 320 - 64) <extra_id_0> }', 'output': b'bucket.x = 320 - 64;'}\n",
            "{'input': b'public String getTitle() throws IOException, StringIndexOutOfBoundsException { Map<String, String> data = getMetadata(); if (data == null || !data.containsKey(\"StreamTitle\")) <extra_id_0> String streamTitle = data.get(\"StreamTitle\"); String artist = streamTitle.substring(streamTitle.indexOf(\"-\") + 1); return artist.trim(); }', 'output': b'{ return \"\"; }'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvADtjcHoGCM"
      },
      "source": [
        "def android_block_preprocessing(ds):\n",
        "  \n",
        "  def to_inputs_and_targets(ex):\n",
        "\n",
        "        inputs = tf.strings.join(['ANDROID_BLOCK:' + ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UY6Cr6OyoGLR",
        "outputId": "82371239-cbe6-423f-d52c-be334bfa8d21"
      },
      "source": [
        "t5.data.TaskRegistry.remove('android_block')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"android_block\",\n",
        "    dataset_fn=nq_android_block,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[android_block_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_android_block\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.data.dataset_providers.FunctionTask at 0x7f60128547f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIpCwr6FjOMd",
        "outputId": "01525f3e-f2d2-433f-b1c7-8e755c9c4a41"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"android_block\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 256, \"targets\": 256})\n",
        "print(\"A few preprocessed training examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "A few preprocessed training examples...\n",
            "{'inputs_pretokenized': b'ANDROID_BLOCK:public T getById(long itemId) { if (itemId != 0) { for (TimelinePage<T> page : pages) { for (T item : page.items) <extra_id_0> } } return getEmptyItem(); }', 'inputs': array([    3, 16446,    15,  3517,    56,  4569,   299,     3, 11955,\n",
            "           5,   288,  9249,     8,     7,    21,    17, 15571,    49,\n",
            "         178,     7,    50,    17,  8430,   410,    25,    70,    29,\n",
            "         715,    58,  7457,     8,     7,    50,    17,    70,   323,\n",
            "          58,   715,     4,  2495,     8, 32099,     6,     6,    14,\n",
            "          41,  1178,   169,    18,     6,     1], dtype=int32), 'targets_pretokenized': b'{ if (item.getId() == itemId) { return item; } }', 'targets': array([   7,   21,   17,  440,    4,  428,   16,   40, 9249,    8,    7,\n",
            "         14,  323,   13,    6,    6,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_BLOCK:private void processRowSelect(View view) { MessageObject message = null; if (view instanceof ChatMessageCell) <extra_id_0> else if (view instanceof ChatActionCell) { message = ((ChatActionCell) view).getMessageObject(); } int type = getMessageType(message); if (type < 2 || type == 20) { return; } addToSelectedMessages(message); updateActionModeTitle(); updateVisibleRows(); }', 'inputs': array([    3, 16446,    15,  3517,    56,  8797,    20,   539,   634,\n",
            "        1495,     5,   143,   403,     8,     7,  1152,    96,   175,\n",
            "          11,    30,    13,    21,    17,   712,   166,     3, 11671,\n",
            "        1148,     8, 32099,    77,    21,    17,   712,   166, 11271,\n",
            "         271,  1148,     8,     7,   175,    11,    17,     5,  3083,\n",
            "         271,  1148,     8,   403,     8,     4,   429,    96,    18,\n",
            "           6,    35,   147,    11,    41,  4493,     5,   301,    10,\n",
            "          21,    17,   171,   136,   804,     3,     2,   147,    40,\n",
            "       17395,     7,    14,    13,     6,  4987,  1390,  1057,     5,\n",
            "         301,    10,   233,  6112,   955,    18,   233,  1404,  2240,\n",
            "          18,     6,     1], dtype=int32), 'targets_pretokenized': b'{ message = ((ChatMessageCell) view).getMessageObject(); }', 'targets': array([    7,   175,    11,    17,     5, 11671,  1148,     8,   403,\n",
            "           8,     4,   429,    96,    18,     6,     1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_BLOCK:public boolean fileExists() { if (file_url == null) { return false; } else <extra_id_0> }', 'inputs': array([    3, 16446,    15,  3517,    56,  4569,    45,   191,  1841,\n",
            "          16,     7,    21,    17,   296,    15,   569,    40,    30,\n",
            "           8,     7,    14,    76,    13,     6,    77, 32099,     6,\n",
            "           1], dtype=int32), 'targets_pretokenized': b'{ File f = new File(file_url); return f.exists(); }', 'targets': array([   7,  181,  328,   11,   24,  181,    5,  296,   15,  569,   10,\n",
            "         14,  328,    4, 1090,   18,    6,    1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_BLOCK:public ArrayList<Boolean> getOpendIndicatorsFromAllLevels(){ ArrayList<Boolean> array=new ArrayList<Boolean>(); Cursor cursor=databaseHelper.getCursorFromAllLevelDatas(); int columnIndex=cursor.getColumnIndex(FeedLevelData.COLUMN_WAS_LEVEL_ALREADY_OPEND); cursor.moveToFirst(); while(cursor.isAfterLast()==false)<extra_id_0> return array; }', 'inputs': array([    3, 16446,    15,  3517,    56,  4569,   173,    25,   649,\n",
            "          29,     3, 13403,   101,  2176,  4368,   517,  7803,   814,\n",
            "         173,    25,   649,    29,   777,   161,    74,   173,    25,\n",
            "         649,   346,  2406,  1073,   161,  2769,   327,     4, 13534,\n",
            "         253,   517,   377,    99,    22,    18,    35,  3215,   161,\n",
            "        1905,     4,  5601,     5,  1988,   377,    99,     4,  2268,\n",
            "          15,  6450,   113,    15,  4134,    15,  2695,  1802,     2,\n",
            "          15,  4350,   228,    10,  1073,     4,  7482,    18,   317,\n",
            "           5,  1905,     4, 25968,  9103,   348,     8,    25,  4837,\n",
            "          15,   111,    15,   148,    29,    14,   777,    13,     6,\n",
            "           1], dtype=int32), 'targets_pretokenized': b'{ array.add(cursor.getInt(columnIndex)==1); cursor.moveToNext(); }', 'targets': array([    7,   777,     4,    67,     5,  1905,     4,  1320,     5,\n",
            "        4946,     8, 24492,    10,  1073,     4,  9074,    18,     6,\n",
            "           1], dtype=int32)}\n",
            "{'inputs_pretokenized': b'ANDROID_BLOCK:private void buildCursorMapForExistingCollaborators(Collaborator collaborator) { if (collaborator == null) <extra_id_0> IndexReference ref = (IndexReference) mCursors.get(collaborator.getSessionId()); if (ref == null) { return; } Set<Collaborator> row = new HashSet<>(); mRowToCollaborators.put(ref.getIndex(), row); row.add(collaborator); }', 'inputs': array([    3, 16446,    15,  3517,    56,  8797,    20,   501,  1123,\n",
            "         100,   203,  4001, 18991,    22,     5, 18991,     3, 19359,\n",
            "           8,     7,    21,    17, 19359,    40,    30,     8, 32099,\n",
            "           3,   163,   478,  1359,    11,    17,   163,   478,     8,\n",
            "        8808,    22,     4,    33,     5, 19359,     4, 13816,    39,\n",
            "          21,    17,  1391,    40,    30,     8,     7,    14,    13,\n",
            "           6,   300,    25, 18991,    29,   722,    11,    24,   978,\n",
            "         447,    54,   634,   129, 18991,    22,     4,   120,     5,\n",
            "        1391,     4,  4488,    72,   722,    10,   722,     4,    67,\n",
            "           5, 19359,    10,     6,     1], dtype=int32), 'targets_pretokenized': b'{ return; }', 'targets': array([ 7, 14, 13,  6,  1], dtype=int32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0chadICjOT_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og1q86yEG-ti"
      },
      "source": [
        "### Hyper Parameter tuning\n",
        "You can run the HP tuning using the following cells.  \n",
        "Please set the correct path of the variable *MODEL_DIR* (the path to save the pretrained model in), *PATH_GIN_FILE* (the gin file configuration for this HP tuning) and *PRETRAINED_DIR* (the folder that contains the pretrained model).  \n",
        "**Keep attention** to change the *pretrained_model_dir* in finetune step (if you are starting the HP tuning from scratch you have to set the value *PRETRAINED_DIR*, if you are restarting the HP tuning from a previous saved checkpoint you have to set the value *MODEL_DIR*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz1a1TxFNKmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ed5fb6-d6fe-4079-ad06-42569c86fac0"
      },
      "source": [
        "def _rate_num_input_examples(task):\n",
        "  if \"train\" in task.splits:\n",
        "    return float(task.num_input_examples(\"train\"))\n",
        "  elif \"validation\" in task.splits:\n",
        "    return float(task.num_input_examples(\"validation\"))\n",
        "  else:\n",
        "    raise ValueError(\"Task %s does not have a train or validation split.\" % (task.name))\n",
        "\n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"all_tasks\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"all_tasks\",\n",
        "    [\"java_construct\", \"java_token\", \"java_block\", \"android_construct\", \"android_token\", \"android_block\"],\n",
        "    default_rate=_rate_num_input_examples\n",
        "     #default_rate=1.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<t5.seqio.dataset_providers.Mixture at 0x7f60129de860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iSh7F89Zqb6"
      },
      "source": [
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "starter_learning_rate = 0.01\n",
        "end_learning_rate = 1e-06\n",
        "decay_steps = 10000\n",
        "\n",
        "learning_rate_fn = PolynomialDecay(\n",
        "    starter_learning_rate,\n",
        "    decay_steps,\n",
        "    end_learning_rate,\n",
        "    power=0.5)\n",
        "\n",
        "\n",
        "MODEL_SIZE = \"small\" \n",
        "\n",
        "# Set the folder where the checkpoints and all the others information will be writed\n",
        "MODEL_DIR = 'gs://bucket_code_completion/T5_extension/HP_TUNING/polynomial/model'\n",
        "\n",
        "# Specify the pre-trained dir which must contain the pre-trained models, the operative_config.gin file and the checkpoint file as well\n",
        "PRETRAINED_DIR='gs://bucket_code_completion/T5_extension/pretrained_with_masking'\n",
        "\n",
        "\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    learning_rate_schedule = learning_rate_fn,\n",
        "    sequence_length={\"inputs\": 256, \"targets\": 256},\n",
        "    save_checkpoints_steps=5000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oHp5ScE7nf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "356405cd-b981-443c-b31f-b0dd6224bf8e"
      },
      "source": [
        "PATH_GIN_FILE = 'gs://bucket_code_completion/T5_extension/HP_TUNING/polynomial/operative_config.gin'\n",
        "\n",
        "import gin\n",
        "\n",
        "with gin.unlock_config():\n",
        "    gin.parse_config_file(PATH_GIN_FILE)\n",
        "    #RUN FINE-TUNING\n",
        "    FINETUNE_STEPS = 100000\n",
        "    model.finetune(\n",
        "        mixture_or_task_name=\"all_tasks\",\n",
        "        pretrained_model_dir=MODEL_DIR,\n",
        "        finetune_steps=FINETUNE_STEPS\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:system_path_file_exists:gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/operative_config.gin\n",
            "ERROR:root:Path not found: gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/operative_config.gin\n",
            "INFO:root:system_path_file_exists:gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/operative_config.gin\n",
            "ERROR:root:Path not found: gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/operative_config.gin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': graph_options {\n",
            "  rewrite_options {\n",
            "    disable_meta_optimizer: true\n",
            "  }\n",
            "}\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.74.11.226:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.74.11.226:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.74.11.226:8470', '_evaluation_master': 'grpc://10.74.11.226:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=100, num_shards=None, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f60128937f0>}\n",
            "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.74.11.226:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Initializing TPU system (master: grpc://10.74.11.226:8470) to fetch topology for model parallelism. This might take a while.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 4623164952406819385)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, -2755722496729458440)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -5193631835858708890)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, -5900706149224433319)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, -109686473074367726)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -6521082495459059018)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 8903077743228584655)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 419886092251454572)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 6684593126510109234)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 196678002195396453)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, -35922286451712662)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/t5/seqio/preprocessors.py:65: UserWarning: Creating resources inside a function passed to Dataset.map() is not supported. Create each resource outside the function, and capture it inside the function to use it.\n",
            "  _tokenize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:num_cores_per_replica: 1\n",
            "INFO:tensorflow:computation_shape: [1, 1, 1, 1]\n",
            "INFO:tensorflow:num_replicas: 8\n",
            "INFO:tensorflow:device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "INFO:tensorflow:device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_shape=[8] physical_shape=[2, 2, 2]\n",
            "INFO:tensorflow:auto_logical_to_physical_tpu logical_to_physical = [(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 1, 0), (1, 1, 1), (1, 0, 0), (1, 0, 1)]\n",
            "WARNING:tensorflow:SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "INFO:tensorflow:SimdMeshImpl init: Shape[batch=8] LayoutRules{('batch', 'batch'), ('experts', 'batch'), ('vocab', 'model'), ('heads', 'model'), ('ensemble', 'ensemble'), ('d_ff', 'model')}\n",
            "INFO:tensorflow:Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7f6014898f60>\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "WARNING:tensorflow:Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:tensorflow:Create pnum_tensor\n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_000/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_001/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_002/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_003/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_004/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/k                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/o                size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/q                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_001/EncDecAttention/v                size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable decoder/block_005/layer_002/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_000/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_001/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_002/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_003/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_004/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/k                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/o                  size 262144       slice_size 262144       Shape[heads=512, d_model=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/q                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_000/SelfAttention/v                  size 262144       slice_size 262144       Shape[d_model=512, heads=512]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wi/kernel         size 1048576      slice_size 1048576      Shape[d_model=512, d_ff=2048]                               \n",
            "INFO:tensorflow:Variable encoder/block_005/layer_001/DenseReluDense/wo/kernel         size 1048576      slice_size 1048576      Shape[d_ff=2048, d_model=512]                               \n",
            "INFO:tensorflow:Variable shared/embedding                                             size 16449536     slice_size 16449536     Shape[vocab=32128, d_model=512]                             \n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/SelfAttention/relative_attention_bias size 512          slice_size 512          Shape[stacked=2, heads=8, buckets=32]                       \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/SelfAttention/relative_attention_bias\n",
            "INFO:tensorflow:Variable stacked/encoder/block_000/layer_000/layer_norm/scale         size 16384        slice_size 16384        Shape[stacked=32, d_model=512]                              \n",
            "INFO:tensorflow:    encoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    encoder/final_layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_000/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_001/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_002/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_003/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_004/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_000/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_001/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/block_005/layer_002/layer_norm/scale\n",
            "INFO:tensorflow:    decoder/final_layer_norm/scale\n",
            "INFO:tensorflow:Trainable Variables            count: 99      Total size: 60506624         Total slice_size: 60506624       \n",
            "INFO:tensorflow:All Variables                  count: 105     Total size: 60691328         Total slice_size: 60691328       \n",
            "INFO:tensorflow:Counters:\n",
            "allreduce: 5.34e+08\n",
            " allreduce/[0]: 5.34e+08\n",
            "  allreduce/[0]/einsum_op: 4.84e+08\n",
            "  allreduce/[0]/reduce_op: 5.03e+07\n",
            "einsum: 1.93e+13\n",
            "einsum_unique: 1.93e+13\n",
            "output: 1.39e+11\n",
            " output/AddOperation: 2.22e+10\n",
            " output/BinaryOpWithBroadcasting: 1.95e+09\n",
            " output/BroadcastOperation: 7.7e+09\n",
            " output/Constant: 8\n",
            " output/EinsumOperation: 5.94e+10\n",
            " output/ImportOperation: 3.15e+06\n",
            " output/MinMaxOperation: 9.5e+06\n",
            " output/OneHotOperation: 6.52e+09\n",
            " output/RandomOperation: 2.02e+07\n",
            " output/RangeOperation: 4.1e+03\n",
            " output/ReduceOperation: 2e+09\n",
            " output/ReshapeOperation: 6.91e+09\n",
            " output/ScalarAddOperation: 4.99e+08\n",
            " output/ScalarMultiplyOperation: 1.5e+09\n",
            " output/ShiftOperation: 1.31e+05\n",
            " output/SlicewiseOperation: 2.5e+10\n",
            " output/StackOperation: 1.35e+06\n",
            " output/StackedVariable: 1.35e+06\n",
            " output/StopGradient: 4.52e+09\n",
            " output/UnstackOperation: 1.35e+06\n",
            " output/Variable: 4.84e+08\n",
            "output_unique: 1.33e+11\n",
            " output_unique/AddOperation: 2.2e+10\n",
            " output_unique/BinaryOpWithBroadcasting: 1.94e+09\n",
            " output_unique/BroadcastOperation: 7.7e+09\n",
            " output_unique/Constant: 1\n",
            " output_unique/EinsumOperation: 5.69e+10\n",
            " output_unique/ImportOperation: 3.94e+05\n",
            " output_unique/MinMaxOperation: 1.25e+06\n",
            " output_unique/OneHotOperation: 6.34e+09\n",
            " output_unique/RandomOperation: 2.02e+07\n",
            " output_unique/RangeOperation: 512\n",
            " output_unique/ReduceOperation: 1.96e+09\n",
            " output_unique/ReshapeOperation: 6.91e+09\n",
            " output_unique/ScalarAddOperation: 6.42e+07\n",
            " output_unique/ScalarMultiplyOperation: 1.47e+09\n",
            " output_unique/ShiftOperation: 1.31e+05\n",
            " output_unique/SlicewiseOperation: 2.28e+10\n",
            " output_unique/StackOperation: 1.69e+05\n",
            " output_unique/StackedVariable: 1.69e+05\n",
            " output_unique/StopGradient: 4.52e+09\n",
            " output_unique/UnstackOperation: 1.69e+05\n",
            " output_unique/Variable: 6.05e+07\n",
            "variables: 6.07e+07\n",
            " variables/trainable: 6.05e+07\n",
            " variables/untrainable: 1.85e+05\n",
            "INFO:tensorflow:Initializing variables from gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/model.ckpt-276000:\n",
            "INFO:tensorflow:Variables in gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/model.ckpt-276000 but not in graph:\n",
            "INFO:tensorflow:\n",
            "INFO:tensorflow:Variables in graph but not in gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/model.ckpt-276000:\n",
            "INFO:tensorflow:\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Bypassing TPUEstimator hook\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:TPU job name worker\n",
            "INFO:tensorflow:Starting the session.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/model.ckpt-276000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1076: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:773: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "INFO:tensorflow:Initialized dataset iterators in 0 seconds\n",
            "INFO:tensorflow:Installing graceful shutdown hook.\n",
            "INFO:tensorflow:Creating heartbeat manager for ['/job:worker/replica:0/task:0/device:CPU:0']\n",
            "INFO:tensorflow:Configuring worker heartbeat: shutdown_mode: WAIT_FOR_COORDINATOR\n",
            "\n",
            "INFO:tensorflow:Starting infeed thread controller.\n",
            "INFO:tensorflow:Starting outfeed thread controller.\n",
            "INFO:tensorflow:Before copy master to slices.\n",
            "INFO:tensorflow:Done with copy master to slices.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 276000...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 276000 into gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 276000...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (0, 0)\n",
            "INFO:tensorflow:loss = 0.044189453, step = 276100\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044433594, step = 276200 (27.512 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.63478\n",
            "INFO:tensorflow:examples/sec: 930.504\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (2, 24)\n",
            "INFO:tensorflow:loss = 0.044677734, step = 276300 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80284\n",
            "INFO:tensorflow:examples/sec: 973.526\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.052734375, step = 276400 (27.834 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.59262\n",
            "INFO:tensorflow:examples/sec: 919.711\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (4, 47)\n",
            "INFO:tensorflow:loss = 0.047851562, step = 276500 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8029\n",
            "INFO:tensorflow:examples/sec: 973.542\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048339844, step = 276600 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80322\n",
            "INFO:tensorflow:examples/sec: 973.625\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (6, 76)\n",
            "INFO:tensorflow:loss = 0.05029297, step = 276700 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80285\n",
            "INFO:tensorflow:examples/sec: 973.53\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04663086, step = 276800 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80279\n",
            "INFO:tensorflow:examples/sec: 973.513\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045898438, step = 276900 (27.323 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.65988\n",
            "INFO:tensorflow:examples/sec: 936.928\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (9, 1)\n",
            "INFO:tensorflow:loss = 0.043945312, step = 277000 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80257\n",
            "INFO:tensorflow:examples/sec: 973.459\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048339844, step = 277100 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80255\n",
            "INFO:tensorflow:examples/sec: 973.453\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (11, 30)\n",
            "INFO:tensorflow:loss = 0.05053711, step = 277200 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80248\n",
            "INFO:tensorflow:examples/sec: 973.435\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.040039062, step = 277300 (27.204 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67591\n",
            "INFO:tensorflow:examples/sec: 941.034\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (13, 55)\n",
            "INFO:tensorflow:loss = 0.045410156, step = 277400 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80291\n",
            "INFO:tensorflow:examples/sec: 973.546\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.049804688, step = 277500 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80292\n",
            "INFO:tensorflow:examples/sec: 973.548\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (15, 84)\n",
            "INFO:tensorflow:loss = 0.043701172, step = 277600 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80255\n",
            "INFO:tensorflow:examples/sec: 973.454\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.0390625, step = 277700 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.803\n",
            "INFO:tensorflow:examples/sec: 973.568\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04345703, step = 277800 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80263\n",
            "INFO:tensorflow:examples/sec: 973.473\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (18, 8)\n",
            "INFO:tensorflow:loss = 0.051757812, step = 277900 (27.430 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.64565\n",
            "INFO:tensorflow:examples/sec: 933.285\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.043945312, step = 278000 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80284\n",
            "INFO:tensorflow:examples/sec: 973.528\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (20, 37)\n",
            "INFO:tensorflow:loss = 0.042236328, step = 278100 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80281\n",
            "INFO:tensorflow:examples/sec: 973.519\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.043945312, step = 278200 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8027\n",
            "INFO:tensorflow:examples/sec: 973.492\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (22, 66)\n",
            "INFO:tensorflow:loss = 0.049316406, step = 278300 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80293\n",
            "INFO:tensorflow:examples/sec: 973.55\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.05029297, step = 278400 (27.217 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67413\n",
            "INFO:tensorflow:examples/sec: 940.577\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (24, 91)\n",
            "INFO:tensorflow:loss = 0.04736328, step = 278500 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80282\n",
            "INFO:tensorflow:examples/sec: 973.522\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048095703, step = 278600 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80271\n",
            "INFO:tensorflow:examples/sec: 973.495\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.046142578, step = 278700 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80308\n",
            "INFO:tensorflow:examples/sec: 973.588\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (27, 20)\n",
            "INFO:tensorflow:loss = 0.05102539, step = 278800 (27.229 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67264\n",
            "INFO:tensorflow:examples/sec: 940.197\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.046142578, step = 278900 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80282\n",
            "INFO:tensorflow:examples/sec: 973.521\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (29, 45)\n",
            "INFO:tensorflow:loss = 0.044189453, step = 279000 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80298\n",
            "INFO:tensorflow:examples/sec: 973.564\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.052001953, step = 279100 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80254\n",
            "INFO:tensorflow:examples/sec: 973.451\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (31, 74)\n",
            "INFO:tensorflow:loss = 0.04638672, step = 279200 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80264\n",
            "INFO:tensorflow:examples/sec: 973.475\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044921875, step = 279300 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80304\n",
            "INFO:tensorflow:examples/sec: 973.577\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (33, 99)\n",
            "INFO:tensorflow:loss = 0.044433594, step = 279400 (27.251 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66958\n",
            "INFO:tensorflow:examples/sec: 939.411\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048828125, step = 279500 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80298\n",
            "INFO:tensorflow:examples/sec: 973.562\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04663086, step = 279600 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80291\n",
            "INFO:tensorflow:examples/sec: 973.545\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (36, 28)\n",
            "INFO:tensorflow:loss = 0.043945312, step = 279700 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80301\n",
            "INFO:tensorflow:examples/sec: 973.57\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04663086, step = 279800 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.803\n",
            "INFO:tensorflow:examples/sec: 973.567\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (38, 53)\n",
            "INFO:tensorflow:loss = 0.045898438, step = 279900 (27.214 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67469\n",
            "INFO:tensorflow:examples/sec: 940.72\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.052490234, step = 280000 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80289\n",
            "INFO:tensorflow:examples/sec: 973.539\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (40, 82)\n",
            "INFO:tensorflow:loss = 0.05053711, step = 280100 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80297\n",
            "INFO:tensorflow:examples/sec: 973.56\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.049804688, step = 280200 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80321\n",
            "INFO:tensorflow:examples/sec: 973.622\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.052978516, step = 280300 (27.786 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.59887\n",
            "INFO:tensorflow:examples/sec: 921.31\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (43, 5)\n",
            "INFO:tensorflow:loss = 0.048095703, step = 280400 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80297\n",
            "INFO:tensorflow:examples/sec: 973.561\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.040283203, step = 280500 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80312\n",
            "INFO:tensorflow:examples/sec: 973.6\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (45, 34)\n",
            "INFO:tensorflow:loss = 0.045654297, step = 280600 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80285\n",
            "INFO:tensorflow:examples/sec: 973.53\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.049316406, step = 280700 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80317\n",
            "INFO:tensorflow:examples/sec: 973.611\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (47, 63)\n",
            "INFO:tensorflow:loss = 0.04663086, step = 280800 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80296\n",
            "INFO:tensorflow:examples/sec: 973.557\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04345703, step = 280900 (27.775 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.6004\n",
            "INFO:tensorflow:examples/sec: 921.703\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (49, 86)\n",
            "INFO:tensorflow:loss = 0.050048828, step = 281000 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80286\n",
            "INFO:tensorflow:examples/sec: 973.532\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044433594, step = 281100 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80318\n",
            "INFO:tensorflow:examples/sec: 973.613\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 281100...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 281100 into gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:970: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 281100...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (51, 83)\n",
            "INFO:tensorflow:loss = 0.047607422, step = 281200 (34.573 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.89246\n",
            "INFO:tensorflow:examples/sec: 740.469\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.05053711, step = 281300 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80292\n",
            "INFO:tensorflow:examples/sec: 973.547\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04638672, step = 281400 (27.181 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.679\n",
            "INFO:tensorflow:examples/sec: 941.825\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (54, 8)\n",
            "INFO:tensorflow:loss = 0.048339844, step = 281500 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80268\n",
            "INFO:tensorflow:examples/sec: 973.487\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.041503906, step = 281600 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80301\n",
            "INFO:tensorflow:examples/sec: 973.569\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (56, 37)\n",
            "INFO:tensorflow:loss = 0.044921875, step = 281700 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80322\n",
            "INFO:tensorflow:examples/sec: 973.626\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.049804688, step = 281800 (27.223 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67333\n",
            "INFO:tensorflow:examples/sec: 940.372\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (58, 62)\n",
            "INFO:tensorflow:loss = 0.047851562, step = 281900 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80293\n",
            "INFO:tensorflow:examples/sec: 973.55\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048828125, step = 282000 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80302\n",
            "INFO:tensorflow:examples/sec: 973.573\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (60, 91)\n",
            "INFO:tensorflow:loss = 0.042236328, step = 282100 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80256\n",
            "INFO:tensorflow:examples/sec: 973.454\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.05053711, step = 282200 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80289\n",
            "INFO:tensorflow:examples/sec: 973.539\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048339844, step = 282300 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80292\n",
            "INFO:tensorflow:examples/sec: 973.547\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (63, 16)\n",
            "INFO:tensorflow:loss = 0.044433594, step = 282400 (27.203 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67621\n",
            "INFO:tensorflow:examples/sec: 941.11\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04345703, step = 282500 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80311\n",
            "INFO:tensorflow:examples/sec: 973.596\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (65, 45)\n",
            "INFO:tensorflow:loss = 0.045410156, step = 282600 (26.293 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80319\n",
            "INFO:tensorflow:examples/sec: 973.617\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.050048828, step = 282700 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80276\n",
            "INFO:tensorflow:examples/sec: 973.505\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (67, 74)\n",
            "INFO:tensorflow:loss = 0.04345703, step = 282800 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80291\n",
            "INFO:tensorflow:examples/sec: 973.546\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045410156, step = 282900 (37.306 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.6805\n",
            "INFO:tensorflow:examples/sec: 686.209\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (69, 61)\n",
            "INFO:tensorflow:loss = 0.046875, step = 283000 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80269\n",
            "INFO:tensorflow:examples/sec: 973.487\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04736328, step = 283100 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80308\n",
            "INFO:tensorflow:examples/sec: 973.589\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (71, 90)\n",
            "INFO:tensorflow:loss = 0.04296875, step = 283200 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80304\n",
            "INFO:tensorflow:examples/sec: 973.579\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.047851562, step = 283300 (27.193 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.6774\n",
            "INFO:tensorflow:examples/sec: 941.415\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.050048828, step = 283400 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80303\n",
            "INFO:tensorflow:examples/sec: 973.577\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (74, 15)\n",
            "INFO:tensorflow:loss = 0.045410156, step = 283500 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80278\n",
            "INFO:tensorflow:examples/sec: 973.512\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048095703, step = 283600 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80291\n",
            "INFO:tensorflow:examples/sec: 973.545\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (76, 44)\n",
            "INFO:tensorflow:loss = 0.04736328, step = 283700 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80294\n",
            "INFO:tensorflow:examples/sec: 973.554\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.05126953, step = 283800 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80271\n",
            "INFO:tensorflow:examples/sec: 973.493\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (78, 69)\n",
            "INFO:tensorflow:loss = 0.044433594, step = 283900 (27.199 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67668\n",
            "INFO:tensorflow:examples/sec: 941.231\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048828125, step = 284000 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80296\n",
            "INFO:tensorflow:examples/sec: 973.558\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (80, 98)\n",
            "INFO:tensorflow:loss = 0.048339844, step = 284100 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80273\n",
            "INFO:tensorflow:examples/sec: 973.5\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.047851562, step = 284200 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80256\n",
            "INFO:tensorflow:examples/sec: 973.455\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044433594, step = 284300 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80298\n",
            "INFO:tensorflow:examples/sec: 973.563\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (83, 23)\n",
            "INFO:tensorflow:loss = 0.040039062, step = 284400 (27.265 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66775\n",
            "INFO:tensorflow:examples/sec: 938.943\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048828125, step = 284500 (26.300 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80226\n",
            "INFO:tensorflow:examples/sec: 973.38\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (85, 52)\n",
            "INFO:tensorflow:loss = 0.048339844, step = 284600 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8025\n",
            "INFO:tensorflow:examples/sec: 973.441\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044433594, step = 284700 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80247\n",
            "INFO:tensorflow:examples/sec: 973.432\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (87, 81)\n",
            "INFO:tensorflow:loss = 0.048583984, step = 284800 (27.236 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67163\n",
            "INFO:tensorflow:examples/sec: 939.938\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044189453, step = 284900 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8026\n",
            "INFO:tensorflow:examples/sec: 973.467\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048339844, step = 285000 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80257\n",
            "INFO:tensorflow:examples/sec: 973.459\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (90, 6)\n",
            "INFO:tensorflow:loss = 0.050048828, step = 285100 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80253\n",
            "INFO:tensorflow:examples/sec: 973.447\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045410156, step = 285200 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80241\n",
            "INFO:tensorflow:examples/sec: 973.416\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (92, 35)\n",
            "INFO:tensorflow:loss = 0.04296875, step = 285300 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80284\n",
            "INFO:tensorflow:examples/sec: 973.527\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.046875, step = 285400 (27.318 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66061\n",
            "INFO:tensorflow:examples/sec: 937.116\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (94, 60)\n",
            "INFO:tensorflow:loss = 0.048583984, step = 285500 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80251\n",
            "INFO:tensorflow:examples/sec: 973.442\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.049316406, step = 285600 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8026\n",
            "INFO:tensorflow:examples/sec: 973.464\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (96, 89)\n",
            "INFO:tensorflow:loss = 0.048828125, step = 285700 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80257\n",
            "INFO:tensorflow:examples/sec: 973.459\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.05053711, step = 285800 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80254\n",
            "INFO:tensorflow:examples/sec: 973.451\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04736328, step = 285900 (27.275 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66636\n",
            "INFO:tensorflow:examples/sec: 938.589\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (99, 14)\n",
            "INFO:tensorflow:loss = 0.0390625, step = 286000 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80268\n",
            "INFO:tensorflow:examples/sec: 973.486\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.041503906, step = 286100 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80258\n",
            "INFO:tensorflow:examples/sec: 973.46\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (101, 43)\n",
            "INFO:tensorflow:loss = 0.05053711, step = 286200 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80247\n",
            "INFO:tensorflow:examples/sec: 973.431\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 286200...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 286200 into gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 286200...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.047851562, step = 286300 (35.658 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.8045\n",
            "INFO:tensorflow:examples/sec: 717.951\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (103, 36)\n",
            "INFO:tensorflow:loss = 0.045654297, step = 286400 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80263\n",
            "INFO:tensorflow:examples/sec: 973.473\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044433594, step = 286500 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80271\n",
            "INFO:tensorflow:examples/sec: 973.493\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (105, 65)\n",
            "INFO:tensorflow:loss = 0.049072266, step = 286600 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80246\n",
            "INFO:tensorflow:examples/sec: 973.43\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.041992188, step = 286700 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8027\n",
            "INFO:tensorflow:examples/sec: 973.492\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (107, 94)\n",
            "INFO:tensorflow:loss = 0.047607422, step = 286800 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80275\n",
            "INFO:tensorflow:examples/sec: 973.505\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045410156, step = 286900 (27.263 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66802\n",
            "INFO:tensorflow:examples/sec: 939.013\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.05126953, step = 287000 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80291\n",
            "INFO:tensorflow:examples/sec: 973.544\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (110, 19)\n",
            "INFO:tensorflow:loss = 0.043945312, step = 287100 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80299\n",
            "INFO:tensorflow:examples/sec: 973.566\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.046142578, step = 287200 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80249\n",
            "INFO:tensorflow:examples/sec: 973.438\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (112, 48)\n",
            "INFO:tensorflow:loss = 0.044921875, step = 287300 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80297\n",
            "INFO:tensorflow:examples/sec: 973.56\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044433594, step = 287400 (27.222 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67337\n",
            "INFO:tensorflow:examples/sec: 940.383\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (114, 73)\n",
            "INFO:tensorflow:loss = 0.047607422, step = 287500 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80255\n",
            "INFO:tensorflow:examples/sec: 973.452\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048339844, step = 287600 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80282\n",
            "INFO:tensorflow:examples/sec: 973.521\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.052734375, step = 287700 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80304\n",
            "INFO:tensorflow:examples/sec: 973.578\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (117, 2)\n",
            "INFO:tensorflow:loss = 0.04321289, step = 287800 (27.301 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.6627\n",
            "INFO:tensorflow:examples/sec: 937.652\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045654297, step = 287900 (26.301 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8022\n",
            "INFO:tensorflow:examples/sec: 973.364\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (119, 27)\n",
            "INFO:tensorflow:loss = 0.047851562, step = 288000 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80249\n",
            "INFO:tensorflow:examples/sec: 973.438\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04296875, step = 288100 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8024\n",
            "INFO:tensorflow:examples/sec: 973.416\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (121, 56)\n",
            "INFO:tensorflow:loss = 0.04345703, step = 288200 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80269\n",
            "INFO:tensorflow:examples/sec: 973.49\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044921875, step = 288300 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80254\n",
            "INFO:tensorflow:examples/sec: 973.45\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (123, 81)\n",
            "INFO:tensorflow:loss = 0.048095703, step = 288400 (27.319 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66042\n",
            "INFO:tensorflow:examples/sec: 937.069\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048828125, step = 288500 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80277\n",
            "INFO:tensorflow:examples/sec: 973.509\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04663086, step = 288600 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80265\n",
            "INFO:tensorflow:examples/sec: 973.479\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (126, 10)\n",
            "INFO:tensorflow:loss = 0.04736328, step = 288700 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80272\n",
            "INFO:tensorflow:examples/sec: 973.496\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045654297, step = 288800 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80263\n",
            "INFO:tensorflow:examples/sec: 973.474\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (128, 30)\n",
            "INFO:tensorflow:loss = 0.049072266, step = 288900 (28.509 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.50769\n",
            "INFO:tensorflow:examples/sec: 897.969\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04736328, step = 289000 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8026\n",
            "INFO:tensorflow:examples/sec: 973.464\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (130, 59)\n",
            "INFO:tensorflow:loss = 0.048339844, step = 289100 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8031\n",
            "INFO:tensorflow:examples/sec: 973.593\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.043945312, step = 289200 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80285\n",
            "INFO:tensorflow:examples/sec: 973.529\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (132, 88)\n",
            "INFO:tensorflow:loss = 0.044921875, step = 289300 (27.249 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66991\n",
            "INFO:tensorflow:examples/sec: 939.497\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048339844, step = 289400 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80263\n",
            "INFO:tensorflow:examples/sec: 973.473\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04296875, step = 289500 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80316\n",
            "INFO:tensorflow:examples/sec: 973.61\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (135, 13)\n",
            "INFO:tensorflow:loss = 0.045654297, step = 289600 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80287\n",
            "INFO:tensorflow:examples/sec: 973.534\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.050048828, step = 289700 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80289\n",
            "INFO:tensorflow:examples/sec: 973.541\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (137, 42)\n",
            "INFO:tensorflow:loss = 0.04736328, step = 289800 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80243\n",
            "INFO:tensorflow:examples/sec: 973.421\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045898438, step = 289900 (27.376 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.65291\n",
            "INFO:tensorflow:examples/sec: 935.146\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (139, 67)\n",
            "INFO:tensorflow:loss = 0.044189453, step = 290000 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80295\n",
            "INFO:tensorflow:examples/sec: 973.555\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048583984, step = 290100 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80288\n",
            "INFO:tensorflow:examples/sec: 973.538\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (141, 96)\n",
            "INFO:tensorflow:loss = 0.048339844, step = 290200 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80255\n",
            "INFO:tensorflow:examples/sec: 973.452\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.047851562, step = 290300 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80274\n",
            "INFO:tensorflow:examples/sec: 973.501\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045898438, step = 290400 (27.219 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67386\n",
            "INFO:tensorflow:examples/sec: 940.509\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (144, 21)\n",
            "INFO:tensorflow:loss = 0.050048828, step = 290500 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8025\n",
            "INFO:tensorflow:examples/sec: 973.44\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044433594, step = 290600 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80273\n",
            "INFO:tensorflow:examples/sec: 973.498\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (146, 50)\n",
            "INFO:tensorflow:loss = 0.04345703, step = 290700 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80285\n",
            "INFO:tensorflow:examples/sec: 973.53\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044921875, step = 290800 (27.275 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66643\n",
            "INFO:tensorflow:examples/sec: 938.606\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (148, 75)\n",
            "INFO:tensorflow:loss = 0.046142578, step = 290900 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80281\n",
            "INFO:tensorflow:examples/sec: 973.519\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.040527344, step = 291000 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80277\n",
            "INFO:tensorflow:examples/sec: 973.509\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.040527344, step = 291100 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80242\n",
            "INFO:tensorflow:examples/sec: 973.42\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (151, 4)\n",
            "INFO:tensorflow:loss = 0.045898438, step = 291200 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80277\n",
            "INFO:tensorflow:examples/sec: 973.509\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044433594, step = 291300 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80254\n",
            "INFO:tensorflow:examples/sec: 973.449\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 291300...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 291300 into gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 291300...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (153, 0)\n",
            "INFO:tensorflow:loss = 0.044433594, step = 291400 (36.062 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.773\n",
            "INFO:tensorflow:examples/sec: 709.888\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04663086, step = 291500 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80285\n",
            "INFO:tensorflow:examples/sec: 973.531\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (155, 29)\n",
            "INFO:tensorflow:loss = 0.048339844, step = 291600 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80268\n",
            "INFO:tensorflow:examples/sec: 973.485\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.041259766, step = 291700 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80263\n",
            "INFO:tensorflow:examples/sec: 973.472\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (157, 58)\n",
            "INFO:tensorflow:loss = 0.044433594, step = 291800 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80285\n",
            "INFO:tensorflow:examples/sec: 973.528\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.043945312, step = 291900 (27.484 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.63843\n",
            "INFO:tensorflow:examples/sec: 931.437\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (159, 82)\n",
            "INFO:tensorflow:loss = 0.045654297, step = 292000 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80272\n",
            "INFO:tensorflow:examples/sec: 973.496\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04321289, step = 292100 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8026\n",
            "INFO:tensorflow:examples/sec: 973.465\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04736328, step = 292200 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8029\n",
            "INFO:tensorflow:examples/sec: 973.541\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (162, 11)\n",
            "INFO:tensorflow:loss = 0.048095703, step = 292300 (27.451 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.64286\n",
            "INFO:tensorflow:examples/sec: 932.573\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048828125, step = 292400 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80242\n",
            "INFO:tensorflow:examples/sec: 973.421\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (164, 35)\n",
            "INFO:tensorflow:loss = 0.044921875, step = 292500 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80297\n",
            "INFO:tensorflow:examples/sec: 973.561\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045654297, step = 292600 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80243\n",
            "INFO:tensorflow:examples/sec: 973.422\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (166, 64)\n",
            "INFO:tensorflow:loss = 0.041992188, step = 292700 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8028\n",
            "INFO:tensorflow:examples/sec: 973.518\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04638672, step = 292800 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80285\n",
            "INFO:tensorflow:examples/sec: 973.53\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (168, 89)\n",
            "INFO:tensorflow:loss = 0.042236328, step = 292900 (27.384 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.6518\n",
            "INFO:tensorflow:examples/sec: 934.86\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04638672, step = 293000 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80286\n",
            "INFO:tensorflow:examples/sec: 973.533\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04663086, step = 293100 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80284\n",
            "INFO:tensorflow:examples/sec: 973.526\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (171, 18)\n",
            "INFO:tensorflow:loss = 0.044189453, step = 293200 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80252\n",
            "INFO:tensorflow:examples/sec: 973.445\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048339844, step = 293300 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80267\n",
            "INFO:tensorflow:examples/sec: 973.484\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (173, 43)\n",
            "INFO:tensorflow:loss = 0.04296875, step = 293400 (27.328 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.65923\n",
            "INFO:tensorflow:examples/sec: 936.762\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.047851562, step = 293500 (26.300 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80229\n",
            "INFO:tensorflow:examples/sec: 973.385\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (175, 72)\n",
            "INFO:tensorflow:loss = 0.046142578, step = 293600 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80253\n",
            "INFO:tensorflow:examples/sec: 973.448\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.046875, step = 293700 (26.301 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80225\n",
            "INFO:tensorflow:examples/sec: 973.376\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.042236328, step = 293800 (27.288 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66444\n",
            "INFO:tensorflow:examples/sec: 938.096\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (178, 0)\n",
            "INFO:tensorflow:loss = 0.040527344, step = 293900 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80273\n",
            "INFO:tensorflow:examples/sec: 973.499\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045898438, step = 294000 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80251\n",
            "INFO:tensorflow:examples/sec: 973.443\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (180, 29)\n",
            "INFO:tensorflow:loss = 0.048095703, step = 294100 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80272\n",
            "INFO:tensorflow:examples/sec: 973.496\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.043945312, step = 294200 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80251\n",
            "INFO:tensorflow:examples/sec: 973.442\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (182, 58)\n",
            "INFO:tensorflow:loss = 0.053222656, step = 294300 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80244\n",
            "INFO:tensorflow:examples/sec: 973.424\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.050048828, step = 294400 (27.661 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.61531\n",
            "INFO:tensorflow:examples/sec: 925.52\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (184, 82)\n",
            "INFO:tensorflow:loss = 0.051757812, step = 294500 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80288\n",
            "INFO:tensorflow:examples/sec: 973.538\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04736328, step = 294600 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80276\n",
            "INFO:tensorflow:examples/sec: 973.507\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.041259766, step = 294700 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8026\n",
            "INFO:tensorflow:examples/sec: 973.467\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (187, 11)\n",
            "INFO:tensorflow:loss = 0.042236328, step = 294800 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80292\n",
            "INFO:tensorflow:examples/sec: 973.547\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04296875, step = 294900 (27.260 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66838\n",
            "INFO:tensorflow:examples/sec: 939.105\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (189, 36)\n",
            "INFO:tensorflow:loss = 0.047851562, step = 295000 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80241\n",
            "INFO:tensorflow:examples/sec: 973.416\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.043701172, step = 295100 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80313\n",
            "INFO:tensorflow:examples/sec: 973.6\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (191, 65)\n",
            "INFO:tensorflow:loss = 0.04345703, step = 295200 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80269\n",
            "INFO:tensorflow:examples/sec: 973.488\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.041992188, step = 295300 (27.762 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.60198\n",
            "INFO:tensorflow:examples/sec: 922.106\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (193, 88)\n",
            "INFO:tensorflow:loss = 0.03881836, step = 295400 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80296\n",
            "INFO:tensorflow:examples/sec: 973.557\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045654297, step = 295500 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80285\n",
            "INFO:tensorflow:examples/sec: 973.529\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.053466797, step = 295600 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80259\n",
            "INFO:tensorflow:examples/sec: 973.463\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (196, 17)\n",
            "INFO:tensorflow:loss = 0.049316406, step = 295700 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80297\n",
            "INFO:tensorflow:examples/sec: 973.56\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044921875, step = 295800 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80319\n",
            "INFO:tensorflow:examples/sec: 973.617\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (198, 42)\n",
            "INFO:tensorflow:loss = 0.04663086, step = 295900 (27.275 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66636\n",
            "INFO:tensorflow:examples/sec: 938.588\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.042236328, step = 296000 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80289\n",
            "INFO:tensorflow:examples/sec: 973.541\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (200, 71)\n",
            "INFO:tensorflow:loss = 0.042236328, step = 296100 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80287\n",
            "INFO:tensorflow:examples/sec: 973.535\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045654297, step = 296200 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80296\n",
            "INFO:tensorflow:examples/sec: 973.559\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.043945312, step = 296300 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80291\n",
            "INFO:tensorflow:examples/sec: 973.544\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (203, 0)\n",
            "INFO:tensorflow:loss = 0.045898438, step = 296400 (27.213 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67473\n",
            "INFO:tensorflow:examples/sec: 940.731\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 296400...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 296400 into gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 296400...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (204, 98)\n",
            "INFO:tensorflow:loss = 0.053710938, step = 296500 (34.514 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.89738\n",
            "INFO:tensorflow:examples/sec: 741.729\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.042236328, step = 296600 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80303\n",
            "INFO:tensorflow:examples/sec: 973.575\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.040527344, step = 296700 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80246\n",
            "INFO:tensorflow:examples/sec: 973.43\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (207, 27)\n",
            "INFO:tensorflow:loss = 0.05444336, step = 296800 (27.553 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.62925\n",
            "INFO:tensorflow:examples/sec: 929.089\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.041259766, step = 296900 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8029\n",
            "INFO:tensorflow:examples/sec: 973.542\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (209, 51)\n",
            "INFO:tensorflow:loss = 0.049316406, step = 297000 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80317\n",
            "INFO:tensorflow:examples/sec: 973.61\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04638672, step = 297100 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80273\n",
            "INFO:tensorflow:examples/sec: 973.498\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (211, 80)\n",
            "INFO:tensorflow:loss = 0.045898438, step = 297200 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80246\n",
            "INFO:tensorflow:examples/sec: 973.429\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04296875, step = 297300 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80316\n",
            "INFO:tensorflow:examples/sec: 973.609\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048583984, step = 297400 (27.255 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66908\n",
            "INFO:tensorflow:examples/sec: 939.284\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (214, 5)\n",
            "INFO:tensorflow:loss = 0.043945312, step = 297500 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8031\n",
            "INFO:tensorflow:examples/sec: 973.594\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.037841797, step = 297600 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80282\n",
            "INFO:tensorflow:examples/sec: 973.522\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (216, 34)\n",
            "INFO:tensorflow:loss = 0.04663086, step = 297700 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80257\n",
            "INFO:tensorflow:examples/sec: 973.458\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045898438, step = 297800 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80279\n",
            "INFO:tensorflow:examples/sec: 973.516\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (218, 59)\n",
            "INFO:tensorflow:loss = 0.048339844, step = 297900 (27.266 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66758\n",
            "INFO:tensorflow:examples/sec: 938.901\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.049316406, step = 298000 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80271\n",
            "INFO:tensorflow:examples/sec: 973.495\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (220, 88)\n",
            "INFO:tensorflow:loss = 0.041748047, step = 298100 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80293\n",
            "INFO:tensorflow:examples/sec: 973.55\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04248047, step = 298200 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80305\n",
            "INFO:tensorflow:examples/sec: 973.58\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.039794922, step = 298300 (27.236 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67155\n",
            "INFO:tensorflow:examples/sec: 939.918\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (223, 13)\n",
            "INFO:tensorflow:loss = 0.045654297, step = 298400 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80271\n",
            "INFO:tensorflow:examples/sec: 973.494\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044921875, step = 298500 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80276\n",
            "INFO:tensorflow:examples/sec: 973.506\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (225, 42)\n",
            "INFO:tensorflow:loss = 0.04248047, step = 298600 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80265\n",
            "INFO:tensorflow:examples/sec: 973.477\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04248047, step = 298700 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80302\n",
            "INFO:tensorflow:examples/sec: 973.573\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (227, 71)\n",
            "INFO:tensorflow:loss = 0.040283203, step = 298800 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80314\n",
            "INFO:tensorflow:examples/sec: 973.605\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.045654297, step = 298900 (27.598 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.62351\n",
            "INFO:tensorflow:examples/sec: 927.619\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (229, 95)\n",
            "INFO:tensorflow:loss = 0.044433594, step = 299000 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80297\n",
            "INFO:tensorflow:examples/sec: 973.561\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04345703, step = 299100 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80295\n",
            "INFO:tensorflow:examples/sec: 973.556\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044921875, step = 299200 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80261\n",
            "INFO:tensorflow:examples/sec: 973.468\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (232, 24)\n",
            "INFO:tensorflow:loss = 0.044189453, step = 299300 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80265\n",
            "INFO:tensorflow:examples/sec: 973.479\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044921875, step = 299400 (27.235 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67167\n",
            "INFO:tensorflow:examples/sec: 939.946\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (234, 49)\n",
            "INFO:tensorflow:loss = 0.046142578, step = 299500 (26.298 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80259\n",
            "INFO:tensorflow:examples/sec: 973.464\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048583984, step = 299600 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80247\n",
            "INFO:tensorflow:examples/sec: 973.433\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (236, 78)\n",
            "INFO:tensorflow:loss = 0.042236328, step = 299700 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80287\n",
            "INFO:tensorflow:examples/sec: 973.534\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.043945312, step = 299800 (27.326 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.65942\n",
            "INFO:tensorflow:examples/sec: 936.812\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.043945312, step = 299900 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80277\n",
            "INFO:tensorflow:examples/sec: 973.51\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (239, 3)\n",
            "INFO:tensorflow:loss = 0.049316406, step = 300000 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80289\n",
            "INFO:tensorflow:examples/sec: 973.54\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.050048828, step = 300100 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80242\n",
            "INFO:tensorflow:examples/sec: 973.419\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (241, 32)\n",
            "INFO:tensorflow:loss = 0.04296875, step = 300200 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80288\n",
            "INFO:tensorflow:examples/sec: 973.537\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04248047, step = 300300 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8028\n",
            "INFO:tensorflow:examples/sec: 973.517\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (243, 57)\n",
            "INFO:tensorflow:loss = 0.046142578, step = 300400 (27.301 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.66293\n",
            "INFO:tensorflow:examples/sec: 937.709\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044433594, step = 300500 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80276\n",
            "INFO:tensorflow:examples/sec: 973.507\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (245, 86)\n",
            "INFO:tensorflow:loss = 0.042236328, step = 300600 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80289\n",
            "INFO:tensorflow:examples/sec: 973.539\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.04296875, step = 300700 (26.299 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80249\n",
            "INFO:tensorflow:examples/sec: 973.437\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.041259766, step = 300800 (26.294 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80316\n",
            "INFO:tensorflow:examples/sec: 973.61\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (248, 10)\n",
            "INFO:tensorflow:loss = 0.045898438, step = 300900 (27.406 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.6489\n",
            "INFO:tensorflow:examples/sec: 934.118\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.043701172, step = 301000 (26.297 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.8027\n",
            "INFO:tensorflow:examples/sec: 973.491\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (250, 39)\n",
            "INFO:tensorflow:loss = 0.048339844, step = 301100 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80283\n",
            "INFO:tensorflow:examples/sec: 973.525\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.048339844, step = 301200 (26.296 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80288\n",
            "INFO:tensorflow:examples/sec: 973.537\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (252, 68)\n",
            "INFO:tensorflow:loss = 0.045898438, step = 301300 (27.247 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.67018\n",
            "INFO:tensorflow:examples/sec: 939.567\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.046142578, step = 301400 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80298\n",
            "INFO:tensorflow:examples/sec: 973.562\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:Outfeed finished for iteration (254, 93)\n",
            "INFO:tensorflow:loss = 0.049316406, step = 301500 (26.295 sec)\n",
            "INFO:tensorflow:global_step/sec: 3.80308\n",
            "INFO:tensorflow:examples/sec: 973.588\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 301500...\n",
            "INFO:tensorflow:Before Save.\n",
            "INFO:tensorflow:About to write a checkpoint\n",
            "INFO:tensorflow:Saving checkpoints for 301500 into gs://bucket_comment_completion/Matteo/HP_TUNING/polynomial/model/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 301500...\n",
            "INFO:tensorflow:Done writing checkpoint.\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n",
            "INFO:tensorflow:loss = 0.044189453, step = 301600 (34.801 sec)\n",
            "INFO:tensorflow:global_step/sec: 2.87345\n",
            "INFO:tensorflow:examples/sec: 735.602\n",
            "INFO:tensorflow:Enqueue next (100) batch(es) of data to infeed.\n",
            "INFO:tensorflow:Dequeue next (100) batch(es) of data from outfeed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-PgnKbgJd8X"
      },
      "source": [
        "### Evaluate the performances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBXTPr0ke04I"
      },
      "source": [
        "# Use a larger batch size for evaluation, which requires less memory.\n",
        "model.batch_size = 512\n",
        "model.eval(\n",
        "    mixture_or_task_name=\"all_tasks\",\n",
        "    # mixture_or_task_name=\"all_tasks\",\n",
        "    checkpoint_steps=-1 #evaluate only last checkpoint\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}